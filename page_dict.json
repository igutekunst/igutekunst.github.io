{
    "/nav": {
        "title": "nav",
        "date": "2024-09-28",
        "tags": [],
        "publish": true,
        "url": "/nav",
        "path": "nav.md",
        "content": "# Example Site\n\n## Navigation\n- [Blog](blog)\n- [About](about)\n- [Contact](contact)"
    },
    "/contact": {
        "title": "Contact Me",
        "date": "2024-09-28",
        "tags": [],
        "publish": true,
        "url": "/contact",
        "path": "contact.md",
        "content": "# Connect With Me\n\nYou can connect with me in a few ways:\n\n - Connect on [Matrix](https://matrix.org) [@isaac:iamthatiam.org](https://matrix.to/#/@isaac:iamthatiam.org)\n - Comment on this or any page"
    },
    "/./2024-09-17-deploying-django-to-digital-ocean": {
        "title": "Deploying Django to Digital Ocean",
        "date": "2024-09-17",
        "tags": [
            "devops",
            "django"
        ],
        "publish": true,
        "url": "/./2024-09-17-deploying-django-to-digital-ocean",
        "path": "django-deploy-notes.md",
        "content": "Although not strictly part of the [[blog/01-personal-infrastructure]] series, this blog post will discuss building and deploying a basic Django application.\n\n\nSecrets:\n\n- Postgres\n- Django admin\n- Django token\n- SSH Keys\n- Ansible Vault Password\n\nAnsible\n\n2 Docker Images\n\n- Nginx\n- Django\n\nQuestions:\n\nShould .env file be backed into container? No\n - Can be loaded by  `compose.yaml`\n - Possibly store in ansible vault, then copied to production box\n\nWhat server deploys to production\nProbably a DroneCI runner. It will need an SSH deploy key to prod DO box.\n\nDigital Ocean Load Balancer\nDO Box, with docker compose situation. This has nginx, postgres, celery etc.\n\nShould I pay DO for a managed database? Probably\n\nWhat does local development look like?\n\nThere will be a a few environments: \n- Fully local with sqlite\n- Local in compose (possibly should add a DNS situation and run through `gateway`)\n-"
    },
    "/": {
        "title": "Programming Reality",
        "date": "2024-08-31",
        "tags": [],
        "publish": true,
        "url": "/",
        "path": "index.md",
        "content": "Welcome to Programming Reality, a site about changing the nature of reality, one line of code at a time.\n\n\n\n## Featured Posts {: .featured-posts}\n - [[blog/12-how-to-solve-any-problem-and-win]]{: .featured-post}\n - [[blog/01-personal-infrastructure]]{: .featured-post}\n - [[blog/14-bringing-forth-new-ideas]]{: .featured-post}"
    },
    "/about": {
        "title": "About",
        "date": "2024-09-28",
        "tags": [],
        "publish": true,
        "url": "/about",
        "path": "about.md",
        "content": "# About the Site\nThis is a personal website for Isaac Harrison Gutekunst. I've created this site to share my exploration of programming reality. The main focus of this is highly technical articles about computer programming, engineering, and [[philosophy]]. \n\n## Content\n\nThe site is structured primarily as a \"blog\", where I write about projects I am working on, or small pieces of knowledge I've acquired over time. \n\n## Technical Aspects of the Site\n\nThis site is hosted on Github Pages, and generated using a custom static site generator written in Python. It uses a customized [Bootstrap](https://getbootstrap.com) for CSS. It generates the entire site from a collection of markdown files, with the YAML [`frontmatter`](https://dpericich.medium.com/what-is-front-matter-and-how-is-it-used-to-create-dynamic-webpages-9d8dc053b457) and custom syntax being inflated into metadata and content on the site.\n\nI am aware the colors, typography, and layout can be improved. I decided it was more important to get the site out than to get it perfect. If you'd like to help with CSS, don't hesitate to [[contact|contact]] me."
    },
    "/blog/2024-09-28-how-conceive-great-ideas": {
        "title": "How Conceive Great Ideas",
        "date": "2024-09-28",
        "tags": [
            "blog",
            "creativity"
        ],
        "publish": true,
        "url": "/blog/2024-09-28-how-conceive-great-ideas",
        "path": "blog/14-bringing-forth-new-ideas.md",
        "content": "![A tranquil workspace surrounded by nature, symbolizing the creative process, with books and warm light inspiring reflection and ideas emerging from an abstract background.](/images/creative-process-1.webp)\n\n\n\n[Support me On Substack](https://programmingreality.substack.com)\n\n\n\nThe following is an excerpt of an essay I wrote to myself to guide my creative process. I share it with you with the hope it inspires insight into your own creative process.\n\nBefore an idea is conceived, the vessel must be ready to receive it. Some ideas arise as the synthesis of other ideas existing in consciousness. Other ideas seem to spring from the formless void into our awareness. The nature and quality of these ideas appear to be correlated to the nature and quality of the mind that serves as the vessel, and the environment surrounding this mind. If someone finds themselves in a life or death situation, the ideas emerging will likely be those that aid in survival. Similarly, if one is at work, the ideas arising are likely those that will further the objective of work.\n\nTherefore, before calling forth an idea from the depth of your consciousness, bring awareness to your inner and outer environment and take care to arrange your environment in such a way that the ideas that arise in response to it are of the kind and nature you desire.\n\nA desert will only support some kinds of life, while a rainforest will support others.\n\nFor example, if you'd like to think of a fun project to improve the quality of your life at home, take some time steeping in this environment. Pay attention to your life, and how your living space affects it. With time, in response to your attention, an idea will present itself. Perhaps a particular wall is calling out for some artwork? Maybe the sink is a bit old and needs replacing.\n\nLarger ideas require a larger environment to grow. If you'd like to change the world, you must first steep yourself in it. Only then will the light of consciousness bring forth that which flows naturally.\n\nTo be continued..."
    },
    "/blog/2024-08-27-welcome-to-programming-reality": {
        "title": "Welcome to Programming Reality",
        "date": "2024-08-27",
        "tags": [
            "blog",
            "philosophy",
            "devops"
        ],
        "publish": true,
        "url": "/blog/2024-08-27-welcome-to-programming-reality",
        "path": "blog/00-welcome.md",
        "content": "[Read on Substack](https://programmingreality.substack.com/p/welcome-to-programming-reality)\n\n[Read on LinkedIn](https://www.linkedin.com/pulse/welcome-programming-reality-isaac-gutekunst-a02ic/)\n\nWelcome to my website and blog. Join me as I share my exploration of reality through the lens of language, and primarily machine-readable language.\n\nI will group my writing into three or more primary categories:\n\n1) My journey working on long-term projects. Posts in this category will be highly technical, and involve topics like computer programming, distributed systems, databases, networking, protocols and more.\n\n2) General musings on the \"Nature of Reality\". I'll typically tag these as \"philosophy\".\n\n3) Miscellaneous things I'm learning. I'll tag these as \"did-you-know\".\n\nI will kick off this journey by introducing my first long-term project: Building a \"Personal Cloud\", using Infrastructure as Code (IaC) tools and my best attempt at building something production-ready.\n\n## Programming Reality\n\nProgramming Reality as phrase is intended to evoke the idea that reality is flexible, and can be programmed with intention. People have always programmed reality with their words by telling stories. The stories we tell each other shape how we see the world, and perhaps even shape the structure of reality itself. In the current age, the programming is becoming more explicit and less occulted. People write computer programs that capture intentions, and then when placed into the right environment have real measurable affects on the physical world. I can tap my fingers on a piece of glass in a certain pattern and  30 minutes later, someone will bring food to my door. This is the magic of Uber Eats on an iPhone using the Internet.\n\n\n# About Me\nIn my professional career, I've spent over a decade working to build various computer systems, primarily for robotics, consumer electronics and aerospace. I've written code that is likely running in mars, running on a satellite orbiting overhead, running inside the headphones worn by millions of people, and running in many more glamorous and not so glamorous environments.\n\nPersonally, I've always been drawn to look closely at the inner workings of everything. I am drawn to understand how light bulbs function, and also why people go to war. I love looking at the structure of companies, families and societies big and small. I see patterns repeating at every level of abstraction, from the organization of ants, to the arrangement of code within a distributed system. \n\n## Why I'm Publishing this Site\n\nI love the world, and feel like sharing. I like building cool stuff! I think everything about life is amazing, and want to share that!\n\nI have so many ideas in my head that I love and want to see take physical form. I've decided to take the next step and start writing about them publicly."
    },
    "/blog/2024-09-22-deploying-services-like-plex-behind-an-nginx-reverse-proxy": {
        "title": "Deploying Services like Plex Behind an Nginx Reverse Proxy",
        "date": "2024-09-22",
        "tags": [
            "blog",
            "development",
            "infrastructure",
            "ansible"
        ],
        "publish": true,
        "url": "/blog/2024-09-22-deploying-services-like-plex-behind-an-nginx-reverse-proxy",
        "path": "blog/07-nginx-reverse-proxy.md",
        "content": "![Sci-Fi server rooms showing nginx reverse proxy symbolically](/images/nginx-reverse-proxy.webp)\n\n\nIn this post, I describe a method for hosting any straightforward web service behind an [nginx](http://nginx.org) reverse proxy with automatically renewing Let's Encrypt SSL Certificates.\n\nThere are numerous web applications that can be self-hosted. I love checking out [awesome-selfhosted](https://github.com/awesome-selfhosted/awesome-selfhosted) on GitHub for ideas. These services differ in many ways: they are written in different languages, have different methods for hosting, handling SSL, and more. Hosting them easily with an automatically renewing SSL certificate is quite simple using `nginx` in a reverse proxy configuration.\n\n## Architecture\n\n\n![Architecture diagram with internet, proxy and 3 services](/images/drawio/nginx-reverse-proxy.svg)\n\nIn this setup, there is a computer running `nginx` with a public IP address. There is a DNS record for every service behind the proxy. I typically use one DNS A record for the `nginx` reverse proxy and create a `CNAME` record for each service.\n\nThis minimizes the amount of code and potentially vulnerable software running on a publicly accessible computer. Certbot can run in one place, easily updating all SSL certificates, and `nginx` can terminate SSL quickly and reliably.\n\nAll the backend services can therefore run unencrypted on port 80, as they are behind a firewall with private IPs in a trusted environment. This simplifies deploying services, as a bespoke SSL configuration is not required.\n\n#### Security Improvements\n\nAlthough running on port 80 in the private address space is \"fine,\" it would be more secure to use SSL between `nginx` and the services. However, doing this manually is quite annoying, and setting up a proper Certificate Authority and secrets management strategy is beyond the scope of this article. In a later article, I'll describe how to further secure this setup once I have [Ansible Vault](https://www.vaultproject.io) running in a private \"core services\" cluster.\n\n## Step By Step\n\nIf you want to recreate something similar, you may follow these steps:\n\n### 1. DNS Setup\n\nConfigure your DNS provider to have a CNAME or A record for each service that you wish to host, pointing to the `nginx` server. For example, `plex.example.com` should point to the IP address of \"Nginx with Certbot\" in the diagram above.\n\nThis is necessary so that A) you can connect to your service, and B) Certbot can issue a certificate.\n\nFor example:\n\n```\n\tgateway.example.com.     300\tIN\t    A\t123.123.123.123\n\tplex.example.com.        300\tCNAME\tgateway.example.com.\n\tblog.example.com.        300\tCNAME\tgateway.example.com.\n\tother.example.com.       300\tCNAME\tgateway.example.com.\n```\n\n\n## 2. nginx setup\n\n### Prerequisites \n\nThis section assumes that you have `nginx` installed on a server. If not, please install it. Digital Ocean has a comprehensive guide that I recommend: [Installing Nginx on Ubuntu 20.04](https://www.digitalocean.com/community/tutorials/how-to-install-nginx-on-ubuntu-20-04).\n\nThis guide covers more than is strictly necessary, but it's good information to know.\n\n### Configuration\n\nFor each service behind `nginx`, you will need a `server` block somewhere in the collection of files that `nginx` loads for its configuration.\n\nTypically, there is a core configuration file `/etc/nginx/nginx.conf`, as well as one or more **available** sites in `/etc/nginx/sites-available` that are enabled by creating a symlink from `/etc/nginx/sites-enabled`. When `nginx` starts up, it looks at `nginx.conf`, which in turn includes all the files in `sites-enabled` (`include /etc/nginx/modules-enabled/*.conf;`). \n\nFor a basic service, you can start with something like this:\n\n```nginx\nupstream foo_server {\n        server 10.0.0.20:80;\n}\n\nserver {\n    server_name foo.example.com;\n\n    location / {\n        proxy_pass http://foo_server;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n\t}\n\tlisten 80;\n\n}\n\n```\n\n### Notes\nThere are a few things to keep in mind:\n\n1. The upstream `foo_server` must be unique. Multiple upstream directives with the same name is an error\n2. If you are hosting multiple services on one machine, you'll need to make them listen on different ports, rather than port 80.\n3. When you run `certbot --nginx` later, it will edit this file\n4. Some services require some additional configuration options\n\n### Testing and Enabling\n\nOnce you are happy with your base nginx configuration file, create a symlink from `sites-enabled`:\n\n```bash\n$ sudo ln -s /etc/nginx/sites-available/foo /etc/nginx/sites-enabled\n```\n\nTest your configuration by running:\n\n```\n$ nginx -t\nnginx: the configuration file /etc/nginx/nginx.conf syntax is ok\nnginx: configuration file /etc/nginx/nginx.conf test is successful\n```\n\nIf it does not show an output like this, edit the file and keep checking until it passes.\n\nThen reload nginx with\n\n```bash\nnginx -s reload\n2024/09/22 21:31:16 [notice] 1183285#1183285: signal process started\n```\n\nIf all is good, you should be able to access your server at `foo.example.com`.\n\n### Troubleshooting\n\n1. Verify that you can access the \"backend\" via it's internal private IP. \n\n2. check the `nginx` logs\n\n```bash\ntail -f /var/log/nginx/error.log\n```\n\n3. Verify you can ping the backend from the reverse proxy host\n4. Check the logs from the backend\n\n## 3. Get an SSL Certificate that automatically renews\n\n[Certbot](https://certbot.eff.org) in combination with [Let's Encrypt](https://letsencrypt.org) makes it very easy to get a free SSL certificate. Digital Ocean has another great article on this. Check out [How To Secure Nginx with Let's Encrypt on Ubuntu 20.04](https://www.digitalocean.com/community/tutorials/how-to-secure-nginx-with-let-s-encrypt-on-ubuntu-20-04)\n\nIf you just want to jump into it, it's as simple as :\n\n\n### Install Certbot\n\n```bash\nsudo apt install certbot python3-certbot-nginx\n```\n\n\n### Run Certbot\n\n```bash\ncertbot --nginx\n```\n\nFollow the prompts, and you are off to the races.\n\n\n## 4. Review Updated nginx configuration\n\nOpen up the `/etc/nginx/sites-available/foo` in your favorite text editor.\n\n```nginx\nupstream foo_server {\n        server 10.0.0.20:11280;\n}\n\nserver {\n    server_name foo.example.com;\n\n    location /.well-known/acme-challenge/ {\n        root /var/www/certbot;\n    }\n\n    location / {\n        proxy_pass http://foo_server;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n\n\n    listen 443 ssl; # managed by Certbot\n    ssl_certificate /etc/letsencrypt/live/foo.example.com/fullchain.pem; # managed by Certbot\n    ssl_certificate_key /etc/letsencrypt/live/foo.example.com/privkey.pem; # managed by Certbot\n    include /etc/letsencrypt/options-ssl-nginx.conf; # managed by Certbot\n    ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; # managed by Certbot\n\n}\n\n\nserver {\n    if ($host = foo.example.com) {\n        return 301 https://$host$request_uri;\n    } # managed by Certbot\n\n\n    server_name foo.example.com;\n    listen 80;\n    return 404; # managed by Certbot\n\n\n}\n```\n\nNotice that it made a few changes:\n\n1. Added a new `server` block that only listens on port 80 and redirects to `https` on port 443.\n2. Changed the first `server` block to listen on port 443 with SSL enabled.\n3. Added a key, certificate, and two other configuration files.\n\n### Notes for Running Plex Server\n\nTo run Plex efficiently behind an `nginx` reverse proxy, you can make some additional edits to the configuration file.\n\nCheck out [plex nginx.conf](https://github.com/toomuchio/plex-nginx-reverseproxy/blob/master/nginx.conf) on GitHub for an extensively tweaked file.\n\nI've settled on a subset of the tweaks, notably:\n\n- HTTP/2\n- Plex headers\n- WebSocket support\n\nI did not add gzip compression or OCSP stapling, but these seem like good ideas.\n\n```nginx\nupstream plex {\n    server 10.0.0.22:32400;\n}\n\nserver {\n    server_name plex.example.com;\n\n    listen 443 ssl http2; # Added http2\n\n    # Plex-specific configuration\n    client_max_body_size 100M;\n    proxy_buffering off;\n\n    location / {\n        proxy_pass https://plex;\n        proxy_ssl_verify off;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n        proxy_set_header Sec-WebSocket-Extensions $http_sec_websocket_extensions;\n        proxy_set_header Sec-WebSocket-Key $http_sec_websocket_key;\n        proxy_set_header Sec-WebSocket-Version $http_sec_websocket_version;\n\n        # Plex-specific headers\n        proxy_set_header X-Plex-Client-Identifier $http_x_plex_client_identifier;\n        proxy_set_header X-Plex-Device $http_x_plex_device;\n        proxy_set_header X-Plex-Device-Name $http_x_plex_device_name;\n        proxy_set_header X-Plex-Platform $http_x_plex_platform;\n        proxy_set_header X-Plex-Platform-Version $http_x_plex_platform_version;\n        proxy_set_header X-Plex-Product $http_x_plex_product;\n        proxy_set_header X-Plex-Version $http_x_plex_version;\n\n        # WebSocket support\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection \"upgrade\";\n    }\n}\n```\n\n\n### Notes for Ansible or other Configuration Management Setup\n\nThere is nothing you need to do here unless you plan on automatically updating the configuration file using a tool like Ansible. If so, you will need to add some logic to your Ansible role to check for the presence of valid SSL certificates. If valid certificates are found, render the full \"post certbot\" file. If not, render the basic one, run certbot, and then render the \"post certbot\" file."
    },
    "/blog/2024-09-15-personal-infrastructure-part-3:-quality-of-life-improvements-with-justfile-automation": {
        "title": "Personal Infrastructure Part 3: Quality of Life Improvements with Justfile Automation",
        "date": "2024-09-15",
        "tags": [
            "blog",
            "development",
            "infrastructure",
            "ansible",
            "django"
        ],
        "publish": true,
        "url": "/blog/2024-09-15-personal-infrastructure-part-3:-quality-of-life-improvements-with-justfile-automation",
        "path": "blog/03-simple-automation.md",
        "content": "![Image of high tech secure vault](/images/ansible-vault-automation-1.webp)\n\n\nIn this post, I describe how I like to use Justfiles to make running common tasks easier.\n\n## 1. Justfile:\n\nI created a Justfile to streamline the setup and execution of various tasks in our project. \n\nJustfiles are simplified modern alternatives to Makefiles. Here's a quick overview of what our Justfile does:\n\nI prefer them for simple task execution that doesn't require the complexity and dependency resolution of Makefiles. I also like avoiding Makefile syntax whenever possible.\n\n\nThe `Justfile` has recipes for the following:\n\n- Creating and activating a virtual environment.\n- Installing or verifying dependencies (Ansible and Cookiecutter).\n- Generating a vault password for Ansible.\n- Creating secrets for our infrastructure.\n- Setting up the entire environment in one go.\n- Running the Ansible playbook.\n\nThe Justfile simplifies our workflow by encapsulating complex commands into simple, memorable recipes. For example, instead of remembering long command sequences, we can now just run `just setup` to prepare our environment or `just ansible_playbook` to execute our Ansible playbook.\n\nThis approach not only saves time but also reduces the likelihood of errors, ensuring consistency across different development environments and making it easier for team members to contribute to the project.\n\n\n```Justfile\n# Justfile\n\n# Set the shell to bash\nset shell := [\"bash\", \"-cu\"]\n\n# Define a variable for the virtual environment directory\nvenv_dir := \"venv\"\n\n# Recipe to create or activate the virtual environment\nvenv:\n    if [ ! -d {{venv_dir}} ]; then \\\n        python3 -m venv {{venv_dir}}; \\\n    fi\n    . {{venv_dir}}/bin/activate\n\n# Recipe to install or verify Ansible and Cookiecutter are installed\ninstall_dependencies:\n    just venv\n    . {{venv_dir}}/bin/activate\n    if ! pip show ansible > /dev/null 2>&1; then \\\n        pip install ansible; \\\n    else \\\n        echo \"Ansible is already installed\"; \\\n    fi\n    if ! pip show cookiecutter > /dev/null 2>&1; then \\\n        pip install cookiecutter; \\\n    else \\\n        echo \"Cookiecutter is already installed\"; \\\n    fi\n\ncreate_vault_password:\n    python3 physical-server-ansible-playbook/get_vault_pass.py generate\n\ncreate_secrets:\n    python3 physical-server-ansible-playbook/create_secrets.py cir\n\n# Recipe to set up the environment (create venv and install dependencies)\nsetup:\n    just venv\n    just install_dependencies\n    just create_vault_password\n\n# Add more tasks as needed\n\n\nansible_playbook:\n    just setup\n    cd physical-server-ansible-playbook && ansible-playbook playbook.yml\n\nping:\n    just setup\n    cd physical-server-ansible-playbook && ansible -i inventory/hosts all -m ping\n\n```\n\n### 1.1 Makefile\n\nTo really push the automation to the next level, I made a Makefile that install [just](https://github.com/casey/just).\n\n\n```bash\n# Install Just\n\n# Detect the operating system\nUNAME_S := $(shell uname -s)\n\n# Default installation method (for unsupported systems)\ninstall_just_default:\n\t@echo \"Unsupported operating system. Please install Just manually.\"\n\n# macOS installation\nifeq ($(UNAME_S),Darwin)\ninstall_just:\n\t@if command -v port >/dev/null 2>&1; then \\\n\t\tsudo port install just; \\\n\telif command -v brew >/dev/null 2>&1; then \\\n\t\tbrew install just; \\\n\telse \\\n\t\techo \"Neither MacPorts nor Homebrew is installed. Installing Homebrew...\"; \\\n\t\t/bin/bash -c \"$$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"; \\\n\t\tbrew install just; \\\n\tfi\nelse\n\n# Debian-based Linux installation\nifeq ($(UNAME_S),Linux)\ninstall_just:\n\t@if command -v apt-get >/dev/null 2>&1; then \\\n\t\tsudo apt-get update && sudo apt-get install -y curl; \\\n\t\tcurl --proto '=https' --tlsv1.2 -sSf https://just.systems/install.sh | sudo bash -s -- --to /usr/local/bin; \\\n\telse \\\n\t\techo \"This doesn't appear to be a Debian-based system. Please install Just manually.\"; \\\n\tfi\nelse\n\n# Fallback to default installation method\ninstall_just: install_just_default\n\nendif\nendif\n\n.PHONY: install_just install_just_default\n\n```\n\n\n## Next Steps: Creating initial secrets automatically\n\nAfter I can store values securely, I'd like to automate the creation of initial random secrets used for various services.\n\nRead more about it in my post: [[blog/04-initial-secrets]]"
    },
    "/blog/2024-09-14-personal-infrastructure-part-2:-setting-up-secret-storage-for-ansible": {
        "title": "Personal Infrastructure Part 2: Setting up Secret Storage for Ansible",
        "date": "2024-09-14",
        "tags": [
            "blog",
            "development",
            "infrastructure",
            "ansible"
        ],
        "publish": true,
        "url": "/blog/2024-09-14-personal-infrastructure-part-2:-setting-up-secret-storage-for-ansible",
        "path": "blog/02-ansible-secrets.md",
        "content": "![Image of high tech secure vault](/images/ansible-vault-1.webp)\n\n\nIn this post, I'm going to explain one way to store secrets when using Ansible.\n\nAnsible has the ability to encrypt and decrypt data, using what it calls the [Ansible Vault](https://docs.ansible.com/ansible/latest/vault_guide/index.html).\n\n## Introduction\n\nMany services require passwords, keys and other secrets. Some are used to access systems and services outside of the ansible deployment, and many are often randomly generated during the initial setup for use within the deployment.\n\nIn both cases, I like encrypting these using Ansible Vault. To make it a bit smoother, I take advantage of a few Ansible features.\nAfter digging around, and doing this a few times, I've settled on the following technique:\n\n1. I use a Python script to retrieve the key used by Ansible Vault to encrypt and decrypt. \n2. Edit `ansible.cfg` to use this script\n3. Make another python script and Ansible Playbook that create new random secrets for usage within playbooks.\n\n\n## Pieces\n\n### 1. Store Vault Password\n\nI made a simple python script that for storing a password in the system's protected storage. This should work on Windows, MacOS and Linux (in Desktop mode), though I haven't tested on anything except MacOS:\n\n```Python\n#!/usr/bin/env python\n\nimport os\nimport sys\nimport keyring\nimport getpass\nimport argparse\nimport secrets\nimport string\n\nAPP_ENV = os.getenv(\"APP_ENV\",\"development\")\n\nSERVICE_NAME = \"AnsibleVault\"\nACCOUNT_NAME = f\"ansible_vault_password_rev_{APP_ENV}\"\n\ndef get_vault_password():\n    password = keyring.get_password(SERVICE_NAME, ACCOUNT_NAME)\n    return password\n\ndef set_vault_password(generate=False):\n    if generate:\n        password = ''.join(secrets.choice(string.ascii_letters + string.digits + string.punctuation) for _ in range(32))\n        print(\"Generated a new secure password.\")\n    else:\n        password = getpass.getpass(\"Enter New Ansible Vault password: \")\n    keyring.set_password(SERVICE_NAME, ACCOUNT_NAME, password)\n    return password\n\ndef clear_vault_password():\n    keyring.delete_password(SERVICE_NAME, ACCOUNT_NAME)\n    print(\"Ansible Vault password has been cleared.\")\n\nif __name__ == \"__main__\":\n    if len(sys.argv) == 1:\n        password = get_vault_password()\n        if not password:\n            sys.stderr.write(\"No Ansible Vault password found. Please set or generate one.\")\n            sys.exit(1)\n        print(password)\n    else:\n        parser = argparse.ArgumentParser(description=\"Manage Ansible Vault password\")\n        parser.add_argument(\"action\", choices=[\"set\", \"generate\", \"clear\"], help=\"Action to perform\")\n        args = parser.parse_args()\n\n        if args.action == \"set\":\n            set_vault_password()\n            print(\"Ansible Vault password has been set.\")\n        elif args.action == \"generate\":\n            stored_password = keyring.get_password(SERVICE_NAME, ACCOUNT_NAME)\n            if stored_password:\n                print(\"Ansible Vault password already exists. Use set to set it, or clear to clear it.\")\n            else:\n                set_vault_password(generate=True)\n                print(\"Ansible Vault password has been set.\")\n        elif args.action == \"clear\":\n            clear_vault_password()\n```\n\n### Configure Ansible to use Python Script\n\nI edited `ansible.cfg` to use the new python script:\n\n```bash\ncat ansible.cfg                                                                                                                                                                                                                master \u2b06 \u2716 \u25fc\n[defaults]\ninventory = inventory/hosts\nremote_user = ansible_user\nprivate_key_file = ~/.ssh/id_ed25519_aslan_ansible\nhost_key_checking = False\n+++ vault_password_file = get_vault_pass.py\ninterpreter_python = auto_silent\n```\n\n**Important:**  Be sure to make sure `get_vault_pass.py` is executable and has an appropriate `#!/usr/bin/env python` or similar.\n\n\n### Create initial vault password\n\n```bash\n    APP_ENV=staging python3 physical-server-ansible-playbook/get_vault_pass.py generate\n    APP_ENV=development python3 physical-server-ansible-playbook/get_vault_pass.py generate\n    APP_ENV=production python3 physical-server-ansible-playbook/get_vault_pass.py generate\n    \n```"
    },
    "/blog/2024-09-26-when-to-choose-cfengine-over-ansible,-and-other-iac-tool-questions": {
        "title": "When to choose CFEngine over Ansible, and other IaC Tool Questions",
        "date": "2024-09-26",
        "tags": [
            "blog",
            "infrastructure",
            "ansible",
            "IaC"
        ],
        "publish": true,
        "url": "/blog/2024-09-26-when-to-choose-cfengine-over-ansible,-and-other-iac-tool-questions",
        "path": "blog/10-dev-ops-tools-theoretical-shootout.md",
        "content": "![Image of space battle with logos of IaC Tools scattered among spaceships](/images/iac-shootoff.webp)\n\n\nSupport my work be [Subscribing on Substack](https://open.substack.com/pub/programmingreality/p/when-to-choose-cfengine-over-ansible?r=4fahib&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true).\n\n# History of Infrastructure as Code\n\nInfrastructure as Code appears to be a natural evolution of computing and human thinking in general. In this post I explore the history of IaC tools and how to select then for a project.\n\nParallels can be seen in early factories and the idea of [scientific management](https://en.wikipedia.org/wiki/Scientific_management), in chemical engineering, electronics, and software development in general. Each time a new tools is developed, the first people do so for the first time. With time \"best practices\" emerge from the many lessons learned through experimentation, intentional exploration and accidental mishaps.\n\nInfrastructure as Code can perhaps be traced back to earlier ideas of [configuration management](https://en.wikipedia.org/wiki/Scientific_management)\n\nThe wikipedia definition is a good place to start:\n\n> **Configuration management**\u00a0(**CM**) is a management process for establishing and maintaining consistency of a product's performance, functional, and physical attributes with its requirements, design, and operational information throughout its life.\n\nRather than existing as the abstract process, IaC tools are actualized implementations that can are instrumental in  *establishing and maintaining consistency of a product's performance...\"* In this case, the product is the collection of computers under management, and the requirements ideally are input into the code describing the infrastructure, and the tools establish and maintain it using a variety of techniques.\n\nThe chronology of IaC (Infrastructure as Code) and configuration management, CFEngine appeared on the scene far earlier than everything else. \n\nPuppet, Chef, and everything that followed them happened over a decade after the initial CFEngine release.\n\nToday all the tools mentioned below are still being used, though from what I can tell from Github Stars, Ansible and Terraform are winning developer popular culture mindshare. In addition, the [JetBrains DevEcosystem 2023 Survey](https://www.jetbrains.com/lp/devecosystem-2023/devops/) and [StackOverflow 2023 Developer Survey](https://survey.stackoverflow.co/2023/#technology) also show Terraform and Ansible far ahead of Chef, Puppet, and Pulumi. CFEngine is not listed.  I've included Kubernetes in this list, because although it is almost in a product class of its own, it does have an enormous amount of mindshare. In fact, none of the Graduating or Incubating [Cloud Native Compute Foundation projects](https://www.cncf.io/projects/) are listed below, except for Kubernetes.\n\n\n### Chronology\n\n| Tool (with GitHub link)                                | Author/Organization                            | Initial Release Date | Source                                                                                                                                                                             |\n| ------------------------------------------------------ | ---------------------------------------------- | -------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [CFEngine](https://github.com/cfengine/core)           | Mark Burgess                                   | 1993                 | [University of Oslo](https://web.archive.org/web/20130723160143/http://www.iu.hio.no/~mark/papers/cfengine_history.pdf)                                                            |\n| [Puppet](https://github.com/puppetlabs/puppet)         | Luke Kanies (Puppet Labs)                      | 2005                 | [Wikipedia](https://en.wikipedia.org/wiki/Puppet_(software))                                                                                                                       |\n| [Chef](https://github.com/chef/chef)                   | Adam Jacob (OpsCode, now Progress)             | January 2009         | [Chef](https://www.chef.io/blog/announcing-chef)                                                                                                                                   |\n| [Salt Stack](https://github.com/saltstack/salt)        | Thomas S. Hatch (SaltStack, now VMware)        | March 2011           | [GitHub](https://github.com/saltstack/salt/commit/461f6f8989c1eb9f51a12af83f54927952146ed0)                                                                                        |\n| AWS CloudFormation                                     | Amazon Web Services                            | February 2011        | [InfoQ](https://www.infoq.com/news/2011/02/aws-cloudformation/) [Innfoworld](https://www.infoworld.com/article/2275178/amazon-aims-to-make-it-easier-to-build-complex-clouds.html) |\n| [Ansible](https://github.com/ansible/ansible)          | Michael DeHaan                                 | February 2012        | Ansible Blog via [archive.org](https://web.archive.org/web/20151023121850/http://www.ansible.com/blog/2013/12/08/the-origins-of-ansible)                                           |\n| [Terraform](https://github.com/hashicorp/terraform)    | Mitchell Hashimoto (HashiCorp)                 | July 2014            | [RedHat](https://www.redhat.com/en/topics/automation/understanding-ansible-vs-terraform-puppet-chef-and-salt)                                                                      |\n| [Kubernetes](https://github.com/kubernetes/kubernetes) | Google (now Cloud Native Computing Foundation) | June 2014            | [Github](https://github.com/kubernetes/kubernetes/commit/a0abb3815755d6a77eed2d07bb0aa7d255e4e769)                                                                                 |\n| [Pulumi](https://github.com/pulumi/pulumi)             | Joe Duffy (Pulumi, Inc.)                       | June 2018            | [Pulumi](https://www.pulumi.com/about/)                                                                                                                                            |\n|                                                        |                                                |                      |                                                                                                                                                                                    |\n\nTable 2. Chronology of Key IaC Tool\n\nI'll add [Nix](https://nixos.org) as an honorable mention for future exploration. \n\nI group these tools into 5 groups:\n\n1. Genesis: CFEngine\n2. Evolution 1: Puppet, Chef, Salt Stack, AWS CloudFormation\n3. Mature Tools: Ansible, Terraform\n4. Evolutionary Leap: Pulumi IaC\n5. Disruptive Different Approach: Kubernetes\n\nCFEngine paved new ground and can be considered the genesis of the Infrastructure as Code Era. It has proven to be a robust tool for advanced and specific use cases within important \"enterprise\" systems.\n\nPuppet and Chef entered the scene around the same time and appear to promote two different evolutions of the core idea. \n\nPuppet uses a declarative Ruby-based DSL with a client/server model. You declare the state you want, and the clients attempt to make it so.\n\nChef uses a different Ruby-based DSL that is procedural. As such, more steps are required to ensure idempotency, that is, running the tool a second time should be the same as running it once. \n\nAnsible is similar to Chef in that it is procedural at its core and defaults to a \"push\" model, though it supports a \"pull\" model as well.\n\nTerraform somewhat belongs in its own category, as it is focused on provisioning infrastructure and less about configuring the services running on the infrastructure. Although Terraform modules can manage just about anything, the core focus is to deploy infrastructure such as containers, VMs, load balancers, firewalls, etc. \n\nPulumi is a newer entrant that promises to make describing and configuring infrastructure easy and friendly using your programming language of choice. It has similar concepts to Terraform and can, in fact, [import Terraform providers](https://www.pulumi.com/registry/packages/terraform-provider/installation-configuration/).\n\n# Choose a Tool\n\nHow does one choose a tool? Is there a single tool that's best for all scenarios? Do I need to learn all of them?\n\nAll these questions and more will be answered below.\n\nRather than immediately answer the questions, I'll explain my thought process.\n\nFirst, I envision the final outcome of my project. Specifically, I am considering the requirements and architecture outlined in [[blog/08-support-services-overview]]. \n\nThen I look at all the tools out there, using Claude, ChatGPT, and Google to gather information. I collect this information and build a mental model of what each tool actually does, what it's good at \"on paper,\" and match that with my intentions/requirements for the project.\n\nIn my research, I came across the concept of [immutable infrastructure](https://www.hashicorp.com/resources/what-is-mutable-vs-immutable-infrastructure). This terminology may have been introduced by HashiCorp, but it applies to the IaC space as a whole. Specifically, both Terraform and Pulumi are designed around the idea that as much as possible should be immutable, that is, once it is created, it does not change. If a change is needed, an old resource should be removed, and a new one added to replace it. If this concept appeals to you, it seems these tools should be strongly considered for \"Deploying Infrastructure.\" If I refer back to my article [[blog/08-support-services-overview]] and consider my goals, the idea of immutable infrastructure makes a lot of sense.\n\nSpecifically, I'd like to easily create and destroy infrastructure and be able to recreate it on new hardware after any kind of failure. This is only possible if configuration is stored somewhere, and becomes much easier if you know that applying the configuration always results in the hardware and software designated as \"infrastructure\" being in the same immutable state.\n\nWith this in mind, I've made the following table:\n\n| Tool      | Declarative vs Procedural | Pull vs Push | Language        | Main Goals    | GitHub Stars                                                                          |\n| --------- | ------------------------- | ------------ | --------------- | ------------- | ------------------------------------------------------------------------------------- |\n| Ansible   | Procedural                | Push         | YAML            | Configuration | ![GitHub stars](https://img.shields.io/github/stars/ansible/ansible?style=social)     |\n| Terraform | Declarative               | Push         | HCL             | Provisioning  | ![GitHub stars](https://img.shields.io/github/stars/hashicorp/terraform?style=social) |\n| Pulumi    | Declarative               | Push         | Various         | Provisioning  | ![GitHub stars](https://img.shields.io/github/stars/pulumi/pulumi?style=social)       |\n| Chef      | Procedural                | Pull         | Ruby            | Configuration | ![GitHub stars](https://img.shields.io/github/stars/chef/chef?style=social)           |\n| Puppet    | Declarative               | Pull         | Ruby            | Configuration | ![GitHub stars](https://img.shields.io/github/stars/puppetlabs/puppet?style=social)   |\n| CFEngine  | Declarative               | Pull         | Custom Language | Configuration | ![GitHub stars](https://img.shields.io/github/stars/cfengine/core?style=social)       |\n\n\nTable 2. Key attributes of leading IaC tools\n\nLooking at this table, a few things are clear to me:\n\n1. CFEngine, Puppet, and Chef are far less popular. Terraform and Ansible are much more popular than the rest.\n\n2. Ansible could serve both my provisioning and configuration needs; however, from experience, I really don't like it. Idempotency and *declarative* Ansible projects appear to be more of an attempted \"catch up\" as projects like Puppet, Pulumi, Terraform, and CFEngine have clearly demonstrated the value of the declarative approach. However, it \"feels\" clunky and like an afterthought to me.\n3. Terraform and Pulumi have a more limited scope. They both focus on the provisioning of architecture.\n\nBased on this, if I choose not to use Ansible, my choices for configuration are CFEngine, Chef, and Puppet. I'll eliminate CFEngine for being old and unpopular, and I'll eliminate Chef because I value declarative languages for defining infrastructure.\n\nAnother common theme I've seen is that Ansible is considered easy, and these other tools are considered \"more difficult.\" When I look more deeply at the criticism, however, it appears that the more \"difficult\" tools solve real problems, making their learning curve part of the \"inherent complexity,\" whereas using Ansible may be easier at first, but those complexities don't go away just because your tool doesn't have native support for them. I've found myself hacking together strange Ansible roles, support scripts, and more just to get the basics to work.\n\nCheck out my articles on my \"Personal Infrastructure series\" 1-4, as well as my article on Deploying Django for more information:\n\n- [[blog/01-ansible-setup]]\n- [[blog/02-ansible-secrets]]\n- [[blog/03-simple-automation]]\n- [[blog/04-initial-secrets]]\n- [[blog/05-simple-ansible-deploy]]\n\n\nTo finish answering my questions, no I don't think I need to learn all the tools. I will try the tools that appeal to me and fit my requirements, and if they don't work out, I'll return to this article and try something else.\n\n\n# Resources\n\nHere are some biased, though informationally dense articles about these tools from two competitors: \n\n- [Chef, Puppet, Ansible, & Salt vs Pulumi](https://www.pulumi.com/docs/iac/concepts/vs/chef-puppet-etc/) - Pulumi \n- [Understanding Ansible, Terraform, Puppet, Chef, and Salt](https://www.redhat.com/en/topics/automation/understanding-ansible-vs-terraform-puppet-chef-and-salt) - Red Hat"
    },
    "/blog/2024-09-13-personal-infrastructure-part-1:-introduction-and-basic-ansible-setup": {
        "title": "Personal Infrastructure Part 1: Introduction and Basic Ansible Setup",
        "date": "2024-09-13",
        "tags": [
            "blog",
            "development",
            "infrastructure"
        ],
        "publish": true,
        "url": "/blog/2024-09-13-personal-infrastructure-part-1:-introduction-and-basic-ansible-setup",
        "path": "blog/01-ansible-setup.md",
        "content": "![Image of computers and power line infrastructure](images/ansible-1.webp)\n\n\nIn this first step, I'm going to build Ansible skeleton project and test connectivity.  I will explain the motivation, my existing setup, how I setup Ansible, and how I make the process a bit smoother and more secure.\n\n# Introduction and Motivation\n\nEventually I will build out [[blog/01-personal-infrastructure]]. I will have a collection of files that given access to a handful of physical or virtual machines will \"build\" a complete foundation for a personal microservices project. To avoid moving too slowly, I will avoid trying to make any part of this process perfectly generic. It will work with my chosen hardware, software and 3rd party services. I will make some effort so that anyone following alone should be able to recreate something similar. \n\n## My Existing Setup\n\n### Hardware\n\n- [Protectli Vault](https://protectli.com/vault-6-port/), configured with 64GB RAM, and a Samsung 4TB SSD.\n- AMD Threadripper desktop with 128GB RAM, and 8TB of SSD storage.\n- Old MSI laptop with 16GB or RAM and 1 TB of SSD Storage\n- Digital Ocean Intel SSD VM with 4GB of RAM and 100 GB or storage.\n- Smaller Protectli Vault running pfSense\n\n### Existing Use Cases\n\n Right now I host a few personal web services:\n\n - [Plex](plex.tv)\n - My \"spiritual\" website: [i am that i am](https://iamthatiam.org), an \"almost\" static site that uses a bit of Django.\n - A personal [Sentry](sentry.io) instance.\n - A personal [Jenkins](jenkins.io) instance\n - A [Plausible Analytics](https://plausible.io) instance\n - A few more\n\n\n### Network Topology\n\nAll the physical hardware is connected via a gigabit switch behind pfSense router. The pfSense router is running a [Wireguard](https://www.wireguard.com) server that I connect to with my roaming Mac laptop, and iPhone. All these servers are on the `10.0.0.0/24` subnet. All Wireguard clients are on the `10.2.0.0/24` subnet, with a specific tunnel for each one.\n\nMy VM provided by Digital Ocean is running Debian 12. It has a permanent Wireguard tunnel back to the pfSense box. For this website, I will call this box `gateway`.\n\nI'ved named all three physical compute nodes after cats:\n\n 1. [`aslan`](https://en.wikipedia.org/wiki/Aslan) - AMD Threadripper system, and main computer node\n 2. [`green-lion`](https://en.wikipedia.org/wiki/Suns_in_alchemy) - Large Protectli vault\n 3. [`bagheera`](https://en.wikipedia.org/wiki/Bagheera) - Old MSI latop\n\n Click on the links to see a bit behind each name. I must confess, I'm not fully aware of the history of `green-lion` within the field of alchemy, so I hope it doesn't mean something terrible!\n\nAccording to ChatGPT:\n\n>In essence, the Green Lion is a metaphor for transformation, representing both the destructive and creative forces in alchemy.\n\n\n These are running in containers manually deployed using [Docker Compose](https://docs.docker.com/compose/). Most of them are running on `aslan`, with some running in \"high availability\" mode, with containers running on both `aslan` and `green-lion`. An [nginx](nginx.com) reverse proxy runs on `gateway` proxying traffic and terminating SSL using [Let's Encrypt](letsencrypt.org).\n\n I'd like to leave all of these services running with close to zero downtime, while deploying new services using increasingly more advanced techniques, culminating in a platform built on top of Kubernetes.\n\n To do so, I'm going to first get some automation in place to configure and manage these physical servers. Then I'll move the manual configuration of my existing service into Ansible, and then from there will setup CI using [Drone CI](drone.io).\n\n# Installing and testing Ansible\n\n## What is Ansible. Why am I using it?\n\nI've used Ansible a few times to deploy web applications and configure servers. I don't love the giant collection of templated YAML, and yet, it provides too much value to ignore.\n\nAs described on their homepage:\n\n> Ansible is an open source IT automation engine that automates provisioning, configuration management, application deployment, orchestration, and many other IT processes. It is free to use, and the project benefits from the experience and intelligence of its thousands of contributors.\n\nIn my words, Ansible is a tool that let's you write YAML file that describe actions that should be taken on a collection of servers, including copying files, installing software and more. When structured and written well, Ansible \"Playbooks\" are [[idempotent]], and repeatable.\n\nI'm going to be using ansible primarily to manage the physical servers before any additional infrastructure is in place.\n\n\n## Requirements\n\nBefore we can use ansible, we need passwordless sudo ssh access to all nodes.\n\nRepeat this step for all physical nodes that you wish to manage with Ansible. I'm going to target `aslan` and `green-lion` initially, and then maybe move onto `gateway` and `bagheera` later.\n\n### 1. Passwordless sudo ansible_user account\n\n\n1. Create a new user named `ansible_user`:\n\n```\nsudo adduser ansible_user\n```\n\n2. Give `ansible_user` sudo access without requiring a password:\n\n```\necho \"ansible_user ALL=(ALL) NOPASSWD:ALL\" | sudo tee /etc/sudoers.d/ansible_user\n```\n\n3. Set up SSH key authentication for `ansible_user`:\n\n```\nsudo mkdir -p /home/ansible_user/.ssh\nsudo chmod 700 /home/ansible_user/.ssh\nsudo touch /home/ansible_user/.ssh/authorized_keys\nsudo chmod 600 /home/ansible_user/.ssh/authorized_keys\n```\n\n4. Copy your public SSH key into the `authorized_keys` file:\n\n```\nsudo sh -c 'echo \"YOUR_PUBLIC_SSH_KEY\" >> /home/ansible_user/.ssh/authorized_keys'\n```\n\n   Replace `YOUR_PUBLIC_SSH_KEY` with your actual public SSH key.\n\n5. Set proper ownership for the `.ssh` directory and its contents:\n\n```\nsudo chown -R ansible_user:ansible_user /home/ansible_user/.ssh\n```\n\nAfter completing these steps, you should be able to SSH into the server as `ansible_user` using your SSH key, and execute sudo commands without a password prompt.\n\n### 2: Ansible installed on local development machine\n\nTo install Ansible on your local development machine, follow these steps:\n\n1. Create a virtual environment:\n\n```bash\npython3 -m venv ansible-venv\n```\n\n2. Activate the virtual environment:\n\n```bash\nsource ansible-venv/bin/activate\n```\n\n3. Install Ansible within the virtual environment:\n\n```bash\npip install ansible\n```\n\n4. Verify the installation:\n\n```bash\nansible --version\n```\n\nThis approach isolates Ansible and its dependencies in a dedicated environment, preventing conflicts with other Python packages on your system.\n\n## Create Ansible inventory and test connectivity\n\nCreate the following files and directory structure\n\n``` bash\n$ tree physical-server-ansible-playbook\n\u251c\u2500\u2500 ansible.cfg\n\u251c\u2500\u2500 inventory\n\u2502   \u2514\u2500\u2500 hosts\n\u251c\u2500\u2500 playbook.yml\n\u2514\u2500\u2500 roles\n    \u2514\u2500\u2500 hello\n        \u2514\u2500\u2500 tasks\n            \u2514\u2500\u2500 main.yml\n```\n\n```bash\ncat ansible.cfg\n[defaults]\ninventory = inventory/hosts\nremote_user = ansible_user\nprivate_key_file = ~/.ssh/id_ed25519_aslan_ansible\nhost_key_checking = False\ninterpreter_python = auto_silent\n```\n\n```bash\n[lz]\ncat inventory/hosts\naslan ansible_host=aslan\ngreen-lion ansible_host=green-lion\n\n[all:vars]\nansible_user=ansible_user\nansible_ssh_private_key_file=~/.ssh/id_ed25519_aslan_ansible\n```\n\n\nOf note, make sure the ssh key is correct in `ansible.cfg`. Also note that `host_key_checking = False` is a potential security risk. I'm running this on my home LAN so I think I'm good, but just be aware.\n\n\nI've called this group of servers the \"lz\" for landing zone. I'll continue the metaphor, as I \"land\" on a distant planet and begin \"terraforming.\"\n\n\nTo verify this is working you can run the ansible ping command\n\n```bash\nansible all -m ping                                                                                                                                                                                                                 \u2718 1 master \u2b06 \u2731 \u25fc\ngreen-lion | UNREACHABLE! => {\n    \"changed\": false,\n    \"msg\": \"Failed to connect to the host via ssh: ansible_user@10.0.0.22: Permission denied (publickey).\",\n    \"unreachable\": true\n}\naslan | SUCCESS => {\n    \"ansible_facts\": {\n        \"discovered_interpreter_python\": \"/usr/bin/python3.11\"\n    },\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\n```\n\nAs you can see, my connectivity to `green-lion` is not correct. I'll go ahead and make the ansible user on `green-lion` and try again.\n\n\n```bash\nansible all -m ping                                                                                                                                                                                                                     master \u2b06 \u2731 \u25fc\naslan | SUCCESS => {\n    \"ansible_facts\": {\n        \"discovered_interpreter_python\": \"/usr/bin/python3.11\"\n    },\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\ngreen-lion | SUCCESS => {\n    \"ansible_facts\": {\n        \"discovered_interpreter_python\": \"/usr/bin/python3.10\"\n    },\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\n```\n\n\n## Next Steps\n\nAs you may be able to guess simply from the direction of this blog, I like automating things. In my next post I'll describe how a securely store secrets for usage within Ansible playbooks, and how I create initial random secrets usable for passwords and keys for deployed software.\n\nRead more in [[blog/02-ansible-secrets]].\n\n\n## Untested Sketchy Scripts\n\nI made a script for setting up the Ansible user on a remote machine. This assumes that you have SSH access to an account with sudo permissions on the remote server.\n\nThis may break for various reasons, but it has worked for me.\n\n```Python\n#!/usr/bin/env python3\n#!/usr/bin/env python3\nimport os\nimport subprocess\nimport sys\nimport getpass\n\ndef run_ssh_command(hostname, command, control_path=None, use_sudo=False):\n    ssh_command = ['ssh']\n    if control_path:\n        ssh_command.extend(['-S', control_path])\n    \n    if use_sudo:\n        full_command = f\"sudo -S bash -c '{command}'\"\n    else:\n        full_command = command\n\n    ssh_command.extend([hostname, full_command])\n    \n    if use_sudo:\n        try:\n            # First try without password\n            result = subprocess.run(ssh_command, capture_output=True, text=True, input='', timeout=5)\n            if result.returncode != 0:\n                # If that fails, prompt for password\n                sudo_password = getpass.getpass(f\"Enter sudo password for {hostname}: \")\n                result = subprocess.run(ssh_command, capture_output=True, text=True, input=sudo_password + '\\n')\n        except subprocess.TimeoutExpired:\n            print(\"Sudo command timed out. Assuming no password is required.\")\n            result = subprocess.run(ssh_command, capture_output=True, text=True)\n    else:\n        result = subprocess.run(ssh_command, capture_output=True, text=True)\n\n    if result.returncode != 0:\n        print(f\"Error running command: {command}\")\n        print(\"Remote host stderr output:\")\n        print(result.stderr.strip())\n    return result.stdout.strip()\n\ndef get_ssh_keys():\n    ssh_dir = os.path.expanduser(\"~/.ssh\")\n    return [f for f in os.listdir(ssh_dir) if f.endswith(\".pub\")]\n\ndef prompt_for_ssh_key(keys):\n    print(\"Available SSH public keys:\")\n    for i, key in enumerate(keys):\n        print(f\"{i + 1}: {key}\")\n    choice = int(input(\"Select the number of the SSH key to use: \")) - 1\n    return keys[choice]\n\ndef check_user_exists(hostname, control_path):\n    return run_ssh_command(hostname, 'id -u ansible_user', control_path).isdigit()\n\ndef check_sudo_permissions(hostname, control_path):\n    return \"NOPASSWD: ALL\" in run_ssh_command(hostname, 'sudo -l -U ansible_user', control_path, use_sudo=True)\n\ndef check_ssh_key_installed(hostname, control_path, ssh_key):\n    result = run_ssh_command(hostname, f'grep -q \"{ssh_key}\" /home/ansible_user/.ssh/authorized_keys && echo \"Key found\" || echo \"Key not found\"', control_path)\n    return \"Key found\" in result\n\ndef enable_ansible_user(hostname, ssh_key, control_path):\n    user_exists = check_user_exists(hostname, control_path)\n    if user_exists:\n        if check_sudo_permissions(hostname, control_path):\n            print(f\"User ansible_user already exists with appropriate sudo permissions.\")\n        else:\n            print(f\"User ansible_user exists but does not have appropriate sudo permissions.\")\n            run_ssh_command(hostname, 'echo \"ansible_user ALL=(ALL) NOPASSWD:ALL\" | sudo tee /etc/sudoers.d/ansible_user', control_path, use_sudo=True)\n            print(\"Added sudo permissions for ansible_user.\")\n    else:\n        run_ssh_command(hostname, 'sudo adduser --disabled-password --gecos \"\" ansible_user', control_path, use_sudo=True)\n        run_ssh_command(hostname, 'echo \"ansible_user ALL=(ALL) NOPASSWD:ALL\" | sudo tee /etc/sudoers.d/ansible_user', control_path, use_sudo=True)\n        print(\"Created ansible_user with sudo permissions.\")\n\n    if not check_ssh_key_installed(hostname, control_path, ssh_key):\n        commands = [\n            'sudo mkdir -p /home/ansible_user/.ssh',\n            'sudo chmod 700 /home/ansible_user/.ssh',\n            'sudo touch /home/ansible_user/.ssh/authorized_keys',\n            'sudo chmod 600 /home/ansible_user/.ssh/authorized_keys',\n            f'echo \"{ssh_key}\" | sudo tee -a /home/ansible_user/.ssh/authorized_keys',\n            'sudo chown -R ansible_user:ansible_user /home/ansible_user/.ssh'\n        ]\n        for command in commands:\n            run_ssh_command(hostname, command, control_path, use_sudo=True)\n        print(\"Installed SSH key for ansible_user.\")\n    else:\n        print(\"SSH key already installed for ansible_user.\")\n\nif __name__ == \"__main__\":\n    if len(sys.argv) != 2:\n        print(\"Usage: enable_ansible <hostname>\")\n        sys.exit(1)\n\n    hostname = sys.argv[1]\n    keys = get_ssh_keys()\n    if not keys:\n        print(\"No SSH public keys found in ~/.ssh\")\n        sys.exit(1)\n\n    selected_key = prompt_for_ssh_key(keys)\n    ssh_key_path = os.path.expanduser(f\"~/.ssh/{selected_key}\")\n    with open(ssh_key_path, 'r') as key_file:\n        ssh_key = key_file.read().strip()\n\n    control_path = f\"/tmp/ansible-ssh-{hostname}-22-control\"\n    \n    # Set up the control master connection\n    subprocess.run(['ssh', '-M', '-S', control_path, '-fNT', hostname], check=True)\n    \n    try:\n        enable_ansible_user(hostname, ssh_key, control_path)\n        print(f\"Ansible user enabled on {hostname} with SSH key {selected_key}\")\n    finally:\n        # Close the control master connection\n        subprocess.run(['ssh', '-S', control_path, '-O', 'exit', hostname], check=True)\n\n```"
    },
    "/blog": {
        "title": "index",
        "date": "2024-09-28",
        "tags": [],
        "publish": true,
        "url": "/blog",
        "path": "blog/index.md",
        "content": "# Blog\n\n## Subtitle\n\n### Foo\n\n\n```Python\n\ndef foo():\n    print(\"Hello, world!\")\n```"
    },
    "/blog/2024-09-24-deploying-a-basic-django-site-using-ansible": {
        "title": "Deploying A Basic Django Site using Ansible",
        "date": "2024-09-24",
        "tags": [
            "blog",
            "development",
            "infrastructure",
            "ansible"
        ],
        "publish": true,
        "url": "/blog/2024-09-24-deploying-a-basic-django-site-using-ansible",
        "path": "blog/05-simple-ansible-deploy.md",
        "content": "![Silhouette of man magically controlling computer windows](/images/django-site-1.webp)\n\nIn this post I describe deploying a simple Django based web application using Ansible into a production environment for use as an online storefront for a small business.\n\n## Background\n\nWhen I'm not [[blog/01-personal-infrastructure|building the new internet]], I am also working on a number of other projects and businesses to support myself. One of these is a small online store selling spiritual themed clothing. For this store, I need a basic website that allows people to buy clothing. I could use a one of many online e-commerce platforms, but I want complete control over the site and features, and am choosing to build it from scratch.\n\nThe store will be built using [Django](https://www.djangoproject.com), and accept payments through [Stripe](https://stripe.com/).\n\nThis project will be a good learning experience, and also demonstrate how the \"legacy\" method of deploying applications can be improved.\n\n## The Plan\n\n### \"Human\" Objectives\n\nI like starting all projects with some idea of what \"success\" looks like. For this project, success means:\n\n1. The online store for this business is up and running well\n2. I have a well-written document describing the process\n3. I know what went well, and what can be improved\n\nSome of these words are pulling a lot of weight. Notably, the phrase \"running well\" can mean many things.\n\nI define it as:\n\n1. People can buy shirts successfully\n2. The website does not go down\n3. If it does go down, I will know about it, and can fix it easily\n4. I can make changes easily without worrying about breaking anything\n\n### Technical Objectives\n\nAt the end of this endeavor, I'll have created a fully functioning system that is easy to change and maintain. It will be self-contained, and not require me to remain engaged with expert-level focus. \n\nI can distill much of what I've learned through the years to: automate everything. By automating everything, I have to explicitly document every step in the form of the \"code\" that does the automation. This forces me to not let anything slip through the cracks, like a one-off hack I find on stack overflow, or that password I set manually, or file permission I needed to change in a panic.\n\nAt the end of this I project I will have:\n\n1. A folder full of python code that is the core Django web application\n2. A Docker container for the application containing a working Django application\n3. A docker container for an `nginx` proxy for serving static files\n4. Secure management of secrets such as API keys and database passwords\n5. A deployment of these containers to real servers, in \"the cloud\" and physically at home\n5. Tools to monitor the running website\n6. Tools to automate all aspects of the deployment\n\n### Assumptions\n\nI will assume that I (or you the reader) has a fully functioning Django application that follows best practices. \n\nSpecifically, I will assume:\n\n1. The Django application does not have any esoteric requirements, and can easily be run locally with `manage.py runserver`\n2. The application does not have and hard-coded keys, credentials, etc, and instead loads these from the operating systems environment\n3. The application does not serve static files directly. These will be served either by a simple web server\n4. The application has logging and error handling setup. Notably, I like using [Sentry](sentry.io) to monitor and keep tabs on my application\n\n\nIn addition, I will assume:\n\n1. This site will be deployed to \"The Cloud\". I will be deploying to Digital Ocean\n2. The site will use an external load balancer that will handle SSL termination. I will again use one from Digital Ocean\n3. The site will use a managed database hosted by the cloud provider\n\nWith these assumptions out of the way, let's get started!\n\nIf you'd like to learn more about Deploying Django, check out the resources:\n\n- [How to deploy Django](https://docs.djangoproject.com/en/5.1/howto/deployment/)\n- [Django Tutorial Part 11: Deploying Django to production](https://developer.mozilla.org/en-US/docs/Learn/Server-side/Django/Deployment)\n\n# Architectural Overview\n\nHere is my go-to architecture for small, low-traffic sites:\n\n1. Frontend load balancer provided by a cloud hosting provider\n2. One or more containerized applications, running Django, with an `nginx` reverse proxy in front of them handling static files. These are orchestrated using `docker compose`.\n3. Managed database provided by cloud hosting provider\n4. Ansible playbook to set everything up\n\nI'll describe the steps I take below in greater detail. This isn't intended to be a tutorial, just a rough overview. \n\n# Steps\n\n### 1. Do the Annoying Manual Steps First\n\nI like getting something working quickly to get momentum going. For this step, I'll go ahead and do the manual steps needed to deploy a site.\n\n1. Ensure you have a domain name. I like getting mine from [Gandi](http://gandi.net)\n2. Provision a \"Droplet\" in Digital Ocean. Configure it to use an SSK key. See [How to Create a Droplet](https://docs.digitalocean.com/products/droplets/how-to/create/) for more information\n3. Provision a  [Load Balancer](https://docs.digitalocean.com/products/networking/load-balancers/) in Digital Ocean. Specifically, I configure it to [terminate SSL](https://docs.digitalocean.com/products/networking/load-balancers/how-to/ssl-termination/), and automatically get new certificates from [Let's Encrypt](https://letsencrypt.org) automatically. This is easy if you [manage your domain](https://docs.digitalocean.com/products/networking/dns/) in Digital Ocean\n4. Connect to your Droplet and do the bare minimum to host something. I like just using `python3 http.server 80` in a folder with a simple `index.html`. You may also want to make a `health` directory with another `index.html` in it so that Digital Ocean will consider this server \"healthy.\"\n5. Configure the Digital Ocean firewall so the web server box is only accessible on port 80 via the load balancer, and your home IP address on port 22 for ssh. Verify you can still SSH into the box, and cannot access it directly via HTTP, while it is still accessible through the load balancer.\n\nHaving done this, I can verify the load balancer and SSL termination are working, and can move on to deploying a working site.\n\n### 2. Setup a Staging Environment\n\nIf you want to go ahead and deploy straight to \"production\", skip this step. I find it very useful to have an environment that is very similar to how I will run the app in production, and use this to test.\n\nIn my case, I have my own custom nginx load balancer, and I setup to SSL terminate and reverse proxy `staging.example.com` \n\nAlternatively, you can spin up another Digital Ocean droplet, and repeat the manual steps with the staging environment.\n\n### 3. Consider Secret Management\n\nFor this site, there are a handful of secrets and configuration values. I keep the non-sensitive ones stored directly in version control, and the sensitive ones encrypted and stored in an [Ansible Vault](https://docs.ansible.com/ansible/latest/vault_guide/index.html) file.\n\nMy Django application itself does not store any secrets, but loads them from [Environment Variables](https://en.wikipedia.org/wiki/Environment_variable)\n\nA Django project loads its runtime settings by importing a [Django settings](https://docs.djangoproject.com/en/5.1/topics/settings/) module. This is a python file with various global variables that are used by other parts of the application.\n\nI have actually created three separate settings modules, a`base` a `local` and a `production`. The `local` and `production` modules import the `base` module, which contains settings that do not change between environments.\n\nThe `local` and `production` modules both use the [python-dotenv](https://pypi.org/project/python-dotenv/) package to load settings from environment variables.\n\nFor example, my `local.py` settings module begins like this:\n\n```Python\nfrom dotenv import load_dotenv\nload_dotenv()\n\nfrom .base import *\n\nDEBUG = True\n\nALLOWED_HOSTS = ['*']\n\n\nSTATIC_URL = '/static/'\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': f'{SITE_NAME}.sqlite',\n    }\n}\n\n```\n\n \n#### Setting Environment Variables at Runtime\n\nTo set environment variables at runtime, the [python-dotenv](https://pypi.org/project/python-dotenv/) looks for a `.env` file in the current working directory, and if it finds one, sets all the described environment variables. For development, I (semi) manually maintain a `.env` file.\n\nFor production, I create a `.env` file during the build process from a template file, and copy it to the production server during the deployment phase. Prior to creating the container, the container runtime (in this case Docker with Docker Compose) loads the the `.env` file and sets the environment variables directly. This avoids having to mount the `.env` file in the Docker container, and removes one Python dependency. \n\nI am currently debating whether I want to forego the [python-dotenv](https://pypi.org/project/python-dotenv/)  library altogether, and use a different tool to load the `.env` on my development system. \n\n\n\n```yaml\nSECRET_KEY={{django_secret_key}}\nDEBUG=False\nPOSTGRES_PORT=5432\nPOSTGRES_HOST=db\nPOSTGRES_USER={{postgres_user}}\nPOSTGRES_PASSWORD={{postgres_password}}\nPOSTGRES_DB={{postgres_db}}\n\nDJANGO_SETTINGS_MODULE={{site_prefix}}.settings.production\nALLOWED_HOSTS={{allowed_hosts}}\n\nADMIN_USERNAME=admin\nADMIN_PASSWORD={{django_admin_password}}\nADMIN_EMAIL={{admin_email}}\n\nEMAIL_HOST={{email_host}}\nEMAIL_PORT={{email_port}}\nEMAIL_USE_TLS={{email_use_tls}}\nEMAIL_HOST_USER={{email_host_user}}\nEMAIL_PASSWORD={{email_password}}\n\nSTRIPE_API_KEY={{stripe_api_key}}\nSTRIPE_PUB_KEY={{stripe_pub_key}}\n```\n\n### 4. Containerize Application including Static File Hosting\n\n I will deploy this application as a [Docker Container](https://docs.docker.com/get-started/docker-concepts/the-basics/what-is-a-container/). I am choosing deploy this as a \"containerized workload\" because I value the determinism and isolation it gives me. Once I create a Docker image, it will run just about the same anywhere, and won't have (much) unintended behavior due to how other services on the server are configured.\n\nAlso, if I'd like to scale up the application, I can simply deploy more containers to more VMs and not have to worry about manual configuration.\n\nI won't write more about the values of containers, but suffice it to say, after many years of resisting, and crying out \"Have operating systems really failed at their key goal of being a platform to run multiple software applications?\", I have decided to embrace the the value they provide, and save my criticism of operating systems for a future discussion.\n\nTo \"containerize\" an application, I consider a few things:\n\n1. What operating system is required to run the software?\n2. What dependencies does it need? \n3. Does it need other services running? Can and should these be containerized as well?\n4. What data from the \"outside\" will the application need? These are settings that will likely live in a `.env` file.\n\nOnce I answer these questions, I whip up a `Dockerfile` that pulls in a good base operating system, and populate the file with all the steps necessary to install the dependencies that are needed, excluding any services that I will run in different containers.\n\nA `Dockerfile` is kind of like a basic shell script that runs a bunch of commands on container, and allows copying between containers. It differs from a traditional shell script in many ways, a notable one being that each step is potentially cached. Keeping that in mind, I typically arrange my `Dockerfile` so most expensive things happen first on their own lines, and fast things that change often are later on in the file, so as I rebuild the image, most of the early steps can be skipped and retrieved from cache. \n\nHere is my `Dockerfile` for this application:\n\n```Dockerfile\n###########\n# Builder #\n###########\n\n# pull official base image\nFROM python:3.11.4-slim-buster as builder\n\n# set work directory\nWORKDIR /usr/src/app\n\n# set environment variables\nENV PYTHONDONTWRITEBYTECODE=1\nENV PYTHONUNBUFFERED=1\n\n# install system dependencies\nRUN apt-get update && \\\n    apt-get install -y --no-install-recommends gcc \n\n# install python dependencies\nCOPY ./frozen-reqs.txt .\nRUN pip wheel --no-cache-dir --no-deps --wheel-dir /usr/src/app/wheels -r frozen-reqs.txt\n\n##############\n# Main Image #\n##############\n\n# pull official base image\nFROM python:3.11.4-slim-buster\n\n# create directory for the app user\nRUN mkdir -p /home/app\n\n# create the app user\nRUN addgroup --system app && adduser --system --group app\n\n# create the appropriate directories\nENV HOME=/home/app\nENV APP_HOME=/home/app/rev\nRUN mkdir -p $APP_HOME/staticfiles $APP_HOME/mediafiles $APP_HOME/logs\nWORKDIR $APP_HOME\n\n# install dependencies\nRUN apt-get update && apt-get install -y --no-install-recommends netcat ffmpeg\n\nCOPY --from=builder /usr/src/app/wheels /wheels\nCOPY --from=builder /usr/src/app/frozen-reqs.txt .\nRUN pip install --upgrade pip\nRUN pip install --no-cache /wheels/*\n\n# install nodejs\nENV NODE_VERSION=20.11.0\nENV NVM_DIR=$HOME/.nvm\n\n# Install curl and nvm\nRUN apt-get update && apt-get install -y curl && \\\n    curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.1/install.sh | bash\n\n# Install Node.js using nvm\nRUN . \"$NVM_DIR/nvm.sh\" && nvm install ${NODE_VERSION} && \\\n    . \"$NVM_DIR/nvm.sh\" && nvm use ${NODE_VERSION} && \\\n    . \"$NVM_DIR/nvm.sh\" && nvm alias default ${NODE_VERSION}\n\n# Update PATH\nENV PATH=\"$NVM_DIR/versions/node/v${NODE_VERSION}/bin:$PATH\"\n\n# copy project\nCOPY . $APP_HOME\n\n# Create log directory and set permissions\nRUN mkdir -p /home/app/logs && chmod -R 755 /home/app/logs\nRUN chown -R app:app /home/app/logs\n\n\nWORKDIR $APP_HOME/store/frontend\nRUN npm install\nWORKDIR $APP_HOME\n\n\n# Ensure log directory is writable\nRUN chmod -R 755 $APP_HOME/logs\n\nRUN sed -i 's/\\r$//g'  $APP_HOME/entrypoint.prod.sh\nRUN chmod +x  $APP_HOME/entrypoint.prod.sh\nRUN chmod +x  $APP_HOME/migrate.sh\n\n# chown all the files to the app user\nRUN chown -R app:app $APP_HOME\n\n# change to the app user\nUSER app\n\n# run entrypoint.prod.sh\nENTRYPOINT [\"/home/app/rev/entrypoint.prod.sh\"]\n```\n\nThere is a basic entrypoint script that waits for Postgres to be happy before continuing. This avoids a few error messages if the web server comes up first. \n\n```bash\n# entypoint.prod.sh\n\n#!/bin/sh\n\nif [ \"$DATABASE\" = \"postgres\" ]\nthen\n    echo \"Waiting for postgres...\"\n\n    while ! nc -z $SQL_HOST $SQL_PORT; do\n      sleep 0.1\n    done\n\n    echo \"PostgreSQL started\"\nfi\n\nexec \"$@\"\n```\n\n\nThere is much to learn about Dockerfiles, including techniques to optimize both the build time, and the final image size. I may take some time to optimize image size later, which is typically done by purging a bunch of files after the work is done, or using multiple images, one to build the application, and another to be the runtime host of it, that doesn't have all the build dependencies. \n\nIn the above file, one optimization I'd like to make is to not include node in the final image. However, I need it since building the frontend bundle happens during the [migration](https://docs.djangoproject.com/en/5.1/topics/migrations/). I will improve this in the future, and copy the optimized bundle into the image, and not bundle node with it.\n\n#### Static File Hosting\n\nDjango sites in production typically do not serve \"static\" files whose content does not change during runtime. Static files are primarily images, css, Javascript and and any other static content. In a Django site, these are added manually to the project, or are the result of compiling a frontend bundle of Javascript, css and more into production bundles.\n\nI typically serve static files from an [nginx](https://nginx.org/en/) server deployed as another container in front of Django. For a high traffic site, it may make sense to copy static files to a CDN or object store like S3. \n\nTo set this up, I create an extremely simple `Dockerfile`\n\n```Dockerfile\nFROM nginx:1.25\n\nRUN rm /etc/nginx/conf.d/default.conf\nCOPY nginx.conf /etc/nginx\n```\n\nThe `nginx.conf` for this container is as follows:\n\n```\nworker_processes 1;\n\nevents {\n    worker_connections 1024;\n}\n\nhttp {\n    include /etc/nginx/mime.types;\n    default_type application/octet-stream;\n\n    gzip on;\n    gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript;\n    gzip_proxied any;\n    gzip_vary on;\n\n    upstream django {\n        server web:8000;\n    }\n\n    server {\n        listen 80;\n\n        location / {\n            proxy_pass http://django;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header Host $host;\n            proxy_redirect off;\n        }\n\n        location /static/ {\n            alias /home/app/{{ app_prefix }}/staticfiles/;\n        }\n\n        location /media/ {\n            alias /home/app/{{ app_prefix }}/mediafiles/;\n        }\n\n        # Add health check endpoint\n        location /health/ {\n            access_log off;\n            return 200 'healthy';\n            add_header Content-Type text/plain;\n        }\n    }\n}\n```\n\nThe main things of not here are \n\n1. Proxy configuration. Location `/` passes all traffic to Django, and sets some headers. \n2. Static configuration. `/static/` serves files directly from `/home/app/{{ app_prefix }}`. The important part here is to copy the static files to the right location prior to running the app\n3. A health check\n\nI'll write about copying files to the right location in later steps.\n\n[Django static files in Production](https://docs.djangoproject.com/en/5.1/howto/static-files/deployment/)\n\n## Surprise Subsection: Using Docker Compose\n\nIn order to manage the Django application and nginx reverse proxy as a single unit, I use [Docker Compose](https://docs.docker.com/compose/)  Docker Compose allows me to bring up a few containers together, configure how network traffic passes between them, and [mount](https://docs.docker.com/engine/storage/volumes/) shared directories between containers.\n\nFor static files hosting, the important part is to copy all the static files to a folder (volume in Docker lingo) that is accessible to the nginx container.\n\nRather than explain all the details, here is my `docker-compose.yaml` template, edited slightly for brevity.\n\n```yaml\nversion: '3.8'\n\nservices:\n  web:\n    image: {{image_name}}\n    command: gunicorn {{site_prefix}}.wsgi:application --bind 0.0.0.0:8000 --workers 5 --threads 2\n    volumes:\n      - static_volume:/home/app/{{site_prefix}}/staticfiles\n      - media_volume:/home/app/{{site_prefix}}/mediafiles\n      - logs_volume:/home/app/{{site_prefix}}/logs\n    env_file:\n      - .env\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health/\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n    expose:\n      - 8000\n    depends_on:\n      - db\n      - migration\n\n  db:\n    image: postgres:15\n    volumes:\n      - postgres_data:/var/lib/postgresql/data/\n    env_file:\n      - .env\n\n  nginx:\n    image: {{nginx_image}}\n    container_name: {{site_prefix}}-nginx\n    volumes:\n      - static_volume:/home/app/{{site_prefix}}/staticfiles\n      - media_volume:/home/app/{{site_prefix}}/mediafiles\n    ports:\n      - {{nginx_port}}:80\n    restart: unless-stopped\n    depends_on:\n      - web\n\n  migration:\n    image: {{image_name}}\n    volumes:\n      - static_volume:/home/app/{{site_prefix}}/staticfiles\n      - media_volume:/home/app/{{site_prefix}}/mediafiles\n    command: ./migrate.sh\n    env_file:\n      - .env\n    depends_on:\n      - db\n\nvolumes:\n  postgres_data:\n  static_volume:\n  media_volume:\n  logs_volume:\n```\n\nSome important things to notice:\n\n1. There are volumes defined for postgres, static files, media files, logs. The static files volume is shared by the migration container and the nginx container.\n2. All of them use the same `.env` file.\n\nI will discuss migrations next\n### 5. Setup Migrations\n\nDjango has built in support for database [migrations](https://docs.djangoproject.com/en/5.1/topics/migrations/). In essence, when you change the database layout in code, it is necessary to update the actual database and potentially migrate any data from the old layout to the new one.\n\nAs migrations need to happen before the new application is deployed, I run an ephemeral container every time I push an upgrade. It will connect to the database, and run migrations, and also copy static files to the appropriate place for `nginx` to host them.\n\nThe ephemeral container is another instance of the main image, with a custom entrypoint script:\n\n```bash\n#!/bin/sh\n\n#!/bin/bash\n\ncd frontend && npm install && npm run build\ncd ..\npython manage.py collectstatic --noinput\npython manage.py migrate --noinput\n\n```\n\n### 6. Build Docker Images for Production\n\nTo build a docker image for production, simply run the following command:\n\n```bash\ndocker build . -t username/image-name:latest\ndocker push username/image-name:latest\n```\n\nThis can also be done in an automated environment like Drone CI. In this case, rather than running the command directly, you use a plugin.\n\n```YAML\nkind: pipeline\ntype: docker\nname: default\n\ntrigger:\n  branch:\n    - staging\n    - production\n  event:\n    - push\n    - pull_request\n\nsteps:\n  - name: build main docker image\n    image: plugins/docker\n    settings:\n      context: rev\n      username:\n        from_secret: dockerhub_username\n      password:\n        from_secret: dockerhub_password\n      repo: {{ image_name }}-${DRONE_BRANCH}\n      dockerfile: rev/Dockerfile\n      cache_from: {{ image_name }}-${DRONE_BRANCH}\n      tags:\n        - latest\n      when:\n        changeset:\n          - {{ site_prefix }}/Dockerfile\n          - {{ site_prefix }}/**/*\n\n  - name: build nginx docker image\n    image: plugins/docker\n    settings:\n      context: nginx\n      username:\n        from_secret: dockerhub_username\n      password:\n        from_secret: dockerhub_password\n      repo: {{ image_name }}-nginx-${DRONE_BRANCH}\n      dockerfile: nginx/Dockerfile\n      cache_from: {{ image_name }}-nginx-${DRONE_BRANCH}\n      tags:\n        - latest\n      when:\n        changeset:\n          - nginx/Dockerfile\n          - nginx/**/*\n\n  - name: run ansible playbook\n    image: ${username}/ansible:latest\n    environment:\n      SSH_KEY:\n        from_secret: ssh_key_${DRONE_BRANCH}\n      ANSIBLE_HOST_KEY_CHECKING: \"False\"\n      ANSIBLE_VAULT_PASSWORD:\n        from_secret: ansible_vault_password_${DRONE_BRANCH}\n    commands:\n      - mkdir -p /root/.ssh\n      - export APP_ENV=${DRONE_BRANCH}\n      - echo \"$${SSH_KEY}\" > /root/.ssh/id_ed25519_ansible\n      - chmod 600 /root/.ssh/id_ed25519_ansible\n      - echo \"$${ANSIBLE_VAULT_PASSWORD}\" > /tmp/vault_password\n      - ansible-playbook --inventory playbooks/inventories/${DRONE_BRANCH} --vault-password-file /tmp/vault_password playbooks/playbook.yml --tags \"staging,ingress\" -vv\n---\nkind: secret\nname: dockerconfig\nget:\n  path: docker\n  name: config\n```\n\nFor more info see [[blog/06-drone-ci]]\n### 7. Create Ansible Playbook to Automate Deployment\n\nNow that I've done just about everything except actually deploy the site, I'll discuss the \"one last final step\" of setting up Ansible. Ansible is a collection of software for automating the installation, management, maintenance and upgrades on collections of computers. It's main strength is that it supports just about any kind of configuration task through plugins, and a frustratingly complete programming environment implemented in YAML.\n\nAnsible effectively creates a giant \"state\" data structure by parsing a huge collection of files, running plugins, and taking actions. As it builds up this state, it may take actions that update the state of remote servers to ideally configure them to be in a corresponding state to that of the Ansible files. However, despite having the goal of being declarative, Ansible is inherently more procedural language, so making effective \"playbooks\" in Ansible lingo is a bit tricky.\n\n\nThe way I approach Ansible is as follows:\n\n#### Establish Connectivity to Hosts and create Inventory(s)\n\nAnsible operates on an \"inventory\" of computers that it applies \"roles\" to will running \"playbooks.\" I describe this in some depth in the article [[blog/01-ansible-setup]].\n\nOnce I can ping all the hosts, I go ahead and make a dumb \"role\" to see if thing work at all. Typically I'll run `echo $env` and verify the hosts are running commands and picking up the correct variables.\n\n#### Role to Deploy Site\n\nNext I create a simple Ansible role to deploy the site. This role assumes a few things:\n\n1. There is an `env.j2` template file that defines all environment variables need at runtime in terms of Ansible facts\n2. There is a similar `compose.yaml.j2` that becomes the final `docker-compose.yml` file for production\n3. Docker images are available to pull from Dockerhub. See section 7 of this article for more information\n\n```YAML\n- name: Deploy site\n  become: true\n  block:\n    - name: Create project directory\n      ansible.builtin.file:\n        path: \"/opt/containers/ansible/{{ site_name }}\"\n        state: directory\n        mode: '0755'\n        recurse: true\n      become: true\n\n    - name: Copy Docker Compose template\n      ansible.builtin.template:\n        src: \"{{ playbook_dir }}/templates/compose.yaml.j2\"\n        dest: \"/opt/containers/ansible/{{ site_name }}/docker-compose.yml\"\n        mode: '0644'\n      become: true\n\n    - name: Create .env file from template\n      ansible.builtin.template:\n        src: \"{{ playbook_dir }}/templates/.env.j2\"\n        dest: \"/opt/containers/ansible/{{ site_name }}/.env\"\n        mode: '0600'\n      become: true\n\n    - name: Pull Docker images\n      community.docker.docker_compose_v2:\n        project_src: \"/opt/containers/ansible/{{ site_name }}\"\n        files:\n          - \"docker-compose.yml\"\n        pull: always\n        state: present\n      become: true\n\n    - name: Start Docker containers\n      community.docker.docker_compose_v2:\n        project_src: \"/opt/containers/ansible/{{ site_name }}\"\n        files:\n          - \"docker-compose.yml\"\n        state: present\n        pull: always\n      become: true\n\n```\n\n### 7. Configure logging and Error Reporting\n\nI typically like error reporting in my production applications. For that I use [sentry](https://sentry.io) For a Django project, this is as simple as including this in my `settings.py` module:\n\n```Python\nimport sentry_sdk\n\nsentry_sdk.init(\n    dsn=\"https:/some-long-code.my-sentry-domain.com/5,\n    # Set traces_sample_rate to 1.0 to capture 100%\n    # of transactions for performance monitoring.\n    traces_sample_rate=1.0,\n)\n```\n\n\nNow when there is an issue, I can see it via a beautiful web interface, configure alerts and more.\n\n![Screenshot of Sentry.io Web interface showing example issue](images/sentry-screenshot-1.png)\n\n\n# Conclusion\n\nThis is a rough overview of one method of deploying a small Django site to production in Digital Ocean. It hand-waves over a lot of the nasty iteration that is present in many of these steps. I present them like \"you just make this file and it all works\", whereas in reality, I maybe spent 10 hours editing a file, running a build command, waiting 2-5 minutes, observing a failure, and trying again. I hope this post helps you avoid some of the same mistakes I made.\n\n##  Lessons Learned\n\nI made a few mistakes that I'm aware of, and likely more that I am not aware of. I'll document some of the more annoying ones here so you may avoid making similar mistakes.\n\n1. Using a `.env` file that is expected to be in the same directory as the Django application. This means that I need to \"mount\" it at runtime, which is annoying\n2. Not using variables for more parts of scripts and config files. I copied and pasted some of the files from earlier projects. I spent a large amount of time chasing down incorrect configuration stemming from copy paste mistakes\n3. Not using sane default values for variables. There are some config values that have sane default values that rarely need to change. If I don't run Postgres on a non-standard port, I shouldn't need to worry about that variable\n4. Using Ansible at all. Although I get a lot of value from Ansible, it's a slow and painful experience to work on. Even with lots of helpful tools, like an Ansible linter, and LLMs helping me write code, it leaves a lot to be desired. I could write a whole criticism of Ansible, and maybe I will in the future. \n5. Have fun! I've had the most fun when I'm not feeling rushed to finish an article. A little work every day makes progress, even if it's slow at times.\n\n\n\n\nThis post took more words, and simultaneously covered less ground than I had hoped.  If you're interested in any particular aspect of it, please don't hesitate to reach out. I'm open to writing another article or two on any aspects that anyone finds particularly interesting."
    },
    "/blog/2024-09-25-personal-infrastructure-part-5:-core-supporting-services": {
        "title": "Personal Infrastructure Part 5: Core Supporting Services",
        "date": "2024-09-25",
        "tags": [
            "blog",
            "development",
            "infrastructure",
            "ansible"
        ],
        "publish": true,
        "url": "/blog/2024-09-25-personal-infrastructure-part-5:-core-supporting-services",
        "path": "blog/08-support-services-overview.md",
        "content": "![Abstract image of system core.](images/core-services-architecture.webp)\n\nIn this post, I will describe the design of the core supporting services that will support the rest of the Personal Cloud.\n\n## Introduction\n\nThe core supporting services provide the foundation and support for building more advanced services, including Kubernetes clusters, load balancers, and more. They exist before any \"user-facing\" services and make deploying and managing the main services easier. Eventually, I will be running a reliable, highly available set of personal services, including file hosting, calendar, AI services, email, messaging, web hosting, and more.\n\nI will build out this system using commercial off-the-shelf components, somewhat in the \"prosumer\" category, as well as rent virtual servers from various cloud providers.\n\n## Goals\n\nThe goal for this step is to build a core layer of services that will support building out the entire personal cloud. This foundational layer needs to be robust against a variety of failures and provide valuable services to be used later.\n\nSpecifically, I'd like to:\n\n1. Be able to quickly create and destroy the second layer of infrastructure.\n2. Securely manage secrets.\n3. Establish secure communication between all nodes, from bare metal physical servers to ephemeral containers and virtual machines.\n4. Observe what is happening in all services, including core support services, in the form of logs, tracing, error reporting, and more.\n5. Leave existing \"legacy\" infrastructure untouched until it is smoothly migrated.\n\n## Constraints, Requirements, and Considerations\n\n- I'd like this setup to be reliable and relatively easy to use once up.\n- I'd like to use hardware I already have.\n- My home internet connection is not very good.\n- My friend will let me put hardware in his home and use his connection.\n- I will move.\n- Power outages occur.\n- Internet outages occur.\n- Theft or total hardware failure may occur.\n- Disk failures may occur.\n- Brain outages will occur.\n- I have a bunch of \"legacy\" services running, primarily as `docker compose` workloads manually started on `aslan` and `green-lion`. I'd like these not to be disturbed until they can gracefully be migrated to the new system.\n\nFor this proof of concept, I'd like to use the hardware I have. If I can use a mix of different machines with different hardware and OSs and still build a solid foundation, it helps prove this can be done on commodity hardware.\n\nI'd like to utilize normal home internet connections for the majority of the connectivity. Ideally, home internet connections would be better. Perhaps building systems like this will create more demand for better home internet. One can dream.\n\nI have some friends who will let me install \"servers\" in their homes. However, out of respect for their privacy and to minimize the chance they become hacked, no home IP addresses are ever used for public ingress into the network.\n\nI live a semi-nomadic life. I've moved just about every year or two since graduating from college in 2013. Therefore, my personal cloud should move with me with minimal disruption.\n\nTaken together, and chewed primarily by my right brain, these considerations spawn some derived requirements:\n\n1. I should be able to add or remove any single node without any ill effects.\n2. The core of the network should be almost bulletproof, and it should remain possible to fix things in many scenarios. I should be able to SSH into any ground-level server no matter what else I do.\n3. I should be able to create a functionally identical cluster from backups within hours, if not minutes, assuming \"application\" data is instantly available.\n4. There should be backups of all critical data. This includes all configuration for all servers, including keys, topology, Terraform or Pulumi state, as well as any \"application\" data running on the cluster(s).\n\n\n\nIf I shake these requirements around and stir them a few times, a rough design begins to emerge.\n\n\n### Requirement Implementation Table\n\n\n| Requirement                          | Mitigation                                                                                                                                     |\n| ------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------- |\n| Handle power outage                  | Servers auto-boot                                                                                                                              |\n| Handle internet outage               | Multiple ingress nodes, multiple backend servers                                                                                               |\n| Never lose connectivity              | Have all servers join Wireguard P2P mesh, and rarely mess with \"ground level\" servers<br>                                                      |\n| Quickly create and destroy clusters  | Automate deploying clusters. Have core supporting infrastructure to make this easy                                                             |\n| Easily recover from serious failures | Create entire cluster automatically from description in code. Backup all the things. Test that the backups work, and intentionally break stuff |\n| Easy adding and removing of nodes    | Make sure that no node is critical. Make services runnable on any, or at least multiple nodes.                                                 |\n| I will move                          | There should be no requirement on the home internet connection. Don't use static IPs.  Connect everything with Wireguard/Headscale             |\n\n\n\nTaking all these factors into consideration, here's what I've arrived at:\n\n### Physical Server Setup\n\nNote: I consider all of these \"physical\" even though some of them are VPS. They are the lowest level computers my personal cloud will have knowledge of.\n\nI will set up some physical servers at my home. They will be connected to a standard, boring Ethernet switch behind a pfSense router that serves as a Wireguard server. Anything connected to this server, or remotely via Wireguard, is IP routable via a private subnet. Right now, I have `aslan` and `green-lion` connected, and `bagheera` connected and powered off.\n\nI will rent three virtual machines: `gateway`, the primary ingress node, a small VPS hosted geographically near me in Los Angeles; `snow-leopard`, a very disk-heavy dedicated server also located geographically close to me; and `palantir` (name undecided yet, I may pick another cat), a secondary ingress node from a different vendor in a different region.\n\n*Public* traffic will enter my network through one of the two ingress nodes, which will have public IP addresses. A load balancer will route the traffic to the correct backend node. As the architecture advances, much traffic will flow through an API gateway, which will authenticate the agent making the request and pass on headers so that many backend services need not worry about authentication and authorization.\n\n\n### Probable Software Stack\n\n- Setup [Headscale](https://www.headscale.net/) to create a management network for all \"physical\" servers\n- Use [Packer](https://www.packer.io/) to create standardized VM images\n- Use [Terraform](https://www.terraform.io/)  or [Pulumi IaC](https://www.pulumi.com) to start VMs on physical servers.\n- Set up [Nomad](https://www.nomadproject.io/), with clients on all VMs and physical servers, and the Nomad server on one or more VMs.\n- Deploy [Vault](https://www.vaultproject.io/) for secrets management\n- Set up monitoring tools (e.g., [Prometheus](https://prometheus.io/), [Grafana](https://grafana.com/))\n- Implement logging solution (e.g., [ELK Stack](https://www.elastic.co/elastic-stack))\n\n\n\n![Stack diagram showing physical, logical and containerized systems](images/drawio/simple-stack-1.drawio.svg)\n\n\n## Ongoing Thought Process\n\nI am spending a lot of time trying to figure out the best bootstrap dance here. In essence, I am asking myself the question: What is the most repeatable way to wrangle a pile of miscellaneous compute devices located all over the world into a consistent platform that can be built upon with confidence?\n\n## VMs as Building Blocks\n\nIf the same VM image is running on two different hosts, it will behave similarly enough to be considered a stable building block. Whether two different instances running can be treated identically is a different question, ideally one that can be asked programmatically at runtime. For example, each will have different CPUs, RAM, GPU, and potentially other differences.\n\nAs long as these differences can be introspected at runtime, I will consider a running VM a stable building block. Some services may require stable building blocks that are not available, but that is a different problem.\n\n## Known Unknowns\n\nThese are questions or areas that I am aware I have not explored fully.\n\n1. Network-aware scheduling of workloads. I'm not sure how to handle where workloads are scheduled based on the performance of different network segments. For example, I'd like my Plex server to be running in the same building that I live in. This may be manual, or maybe there are [tools](https://scheduler-plugins.sigs.k8s.io/docs/kep/260-network-aware-scheduling/readme/) to automate this scheduling and placement.\n2. Should I use Terraform or Pulumi IaC? How much of the stack should be managed by each of these tools? Should there be multiple instances of them to make a nested platform? For example, should I deploy a Vault instance to a VM and use that to store secrets used to deploy another Vault in a high-availability setup?\n\n## Next Steps\n\n1. Try out Terraform and Pulumi IaC and choose one for now.\n2. Build out the core infrastructure as described here.\n3. Work on the secondary core services. These will include a service mesh, API gateway, unified authentication server, and possibly a Kubernetes cluster for fun."
    },
    "/blog/2024-09-23-setting-up-continuous-integration-using-drone-ci": {
        "title": "Setting up Continuous Integration using Drone CI",
        "date": "2024-09-23",
        "tags": [
            "blog",
            "development",
            "infrastructure",
            "ansible"
        ],
        "publish": true,
        "url": "/blog/2024-09-23-setting-up-continuous-integration-using-drone-ci",
        "path": "blog/06-drone-ci.md",
        "content": "![Sci-Fi Space Loading dock with Drone CI Imagery](/images/drone-ci.webp)\n\n\nIn this post I setup Drone CI to build my code automatically when pushed to Github.\n\nI'm choosing [Drone CI](https://docs.drone.io) because it is easy to deploy via Docker, and supports defining builds using a simple YAML syntax.\n\nThis is a brief blog post, giving me time to write about the painful process of deploying a site using Ansible alone, which will pave the way for a streamlined process using Terraform later.\n\n\n## Install Drone Control Node\n\nSteps:\n\n1. Select a node to run Drone CI on. Setup DNS and [[blog/07-nginx-reverse-proxy|SSL termination via a load balancer]] if desired.\n2. Whip up a `compose.yaml` file based on their example command line invocation\n\n```YAML\nversion: '3'\n\nservices:\n  drone:\n    image: drone/drone:2\n    container_name: drone\n    volumes:\n      - /var/lib/drone:/data\n    environment:\n      - DRONE_GITHUB_CLIENT_ID=xxxx\n      - DRONE_GITHUB_CLIENT_SECRET=xxxx\n      - DRONE_RPC_SECRET=xxxx\n      - DRONE_SERVER_HOST=drone.example.com\n      - DRONE_SERVER_PROTO=https\n      - DRONE_USER_FILTER=xxxx\n    ports:\n      - \"11080:80\"\n      - \"11443:443\"\n    restart: always\n```\n\n3. Fire it up with `docker compose up -d`\n4. Fire up a runner node by creating a similar `compose.yaml` for the runner:\n\n```YAML\nservices:\n  runner:\n    image: drone/drone-runner-docker:1\n    container_name: runner\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n    environment:\n      - DRONE_RPC_PROTO=https\n      - DRONE_RPC_HOST=drone.example.com\n      - DRONE_RPC_SECRET=xxxx\n      - DRONE_RUNNER_CAPACITY=2\n      - DRONE_RUNNER_NAME=runner-1\n    ports:\n      - \"3000:3000\"\n    restart: always\n```\n\n# Start a Drone Builder\n\n Start a Drone builder by creating `.drone.yml`. You may want to store some secrets in the Drone CI environment. This can be done via the web interface, or via CLI.\n\nThe code below is a slightly edited version I created for deploying a simple Django application. It only builds the pipeline for the `production` and `staging` branches.\n\n```YAML\n\nkind: pipeline\ntype: docker\nname: default\n\ntrigger:\n  branch:\n    - staging\n    - production\n  event:\n    - push\n    - pull_request\n\nsteps:\n  - name: build main docker image\n    image: plugins/docker\n    settings:\n      context: rev\n      username:\n        from_secret: dockerhub_username\n      password:\n        from_secret: dockerhub_password\n      repo: ${IMAGE_NAME}-${DRONE_BRANCH}\n      dockerfile: rev/Dockerfile\n      cache_from: ${IMAGE_NAME}-${DRONE_BRANCH}\n      tags:\n        - latest\n      when:\n        changeset:\n          - rev/Dockerfile\n          - rev/**/*\n\n  - name: build nginx docker image\n    image: plugins/docker\n    settings:\n      context: nginx\n      username:\n        from_secret: dockerhub_username\n      password:\n        from_secret: dockerhub_password\n      repo: ${IMAGE_NAME}-${DRONE_BRANCH}\n      dockerfile: nginx/Dockerfile\n      cache_from: ${IMAGE_NAME}-nginx-${DRONE_BRANCH}\n      tags:\n        - latest\n      when:\n        changeset:\n          - nginx/Dockerfile\n          - nginx/**/*\n\n  - name: run ansible playbook\n    image: scootekunst/ansible:latest\n    environment:\n      SSH_KEY:\n        from_secret: ssh_key_${DRONE_BRANCH}\n      ANSIBLE_HOST_KEY_CHECKING: \"False\"\n      ANSIBLE_VAULT_PASSWORD:\n        from_secret: ansible_vault_password_${DRONE_BRANCH}\n    commands:\n      - mkdir -p /root/.ssh\n      - export APP_ENV=${DRONE_BRANCH}\n      - echo \"$${SSH_KEY}\" > /root/.ssh/id_ed25519_drone\n      - chmod 600 /root/.ssh/id_ed25519_drone\n      - echo \"$${ANSIBLE_VAULT_PASSWORD}\" > /tmp/vault_password\n      - cat /tmp/vault_password | base64\n      - ansible-playbook --inventory playbooks/inventories/${DRONE_BRANCH} --vault-password-file /tmp/vault_password playbooks/playbook.yml --tags \"staging,ingress\" -vv\n---\nkind: secret\nname: dockerconfig\nget:\n  path: docker\n  name: config\n```\n\n\n## Off to the races! \n\nWith these steps, every time I push to the `staging` or `production` branch, Drone will build two docker images and run my Ansible playbook, deploying my site to a production or staging environment."
    },
    "/blog/2024-09-16-personal-infrastructure-part-4:-creating-and-storing-initial-secrets": {
        "title": "Personal Infrastructure Part 4: Creating and Storing Initial Secrets",
        "date": "2024-09-16",
        "tags": [
            "blog",
            "development",
            "infrastructure",
            "ansible"
        ],
        "publish": true,
        "url": "/blog/2024-09-16-personal-infrastructure-part-4:-creating-and-storing-initial-secrets",
        "path": "blog/04-initial-secrets.md",
        "content": "![Image of secrets being created in space](/images/ansible-vault-secrets-1.webp)\n\n\nIn this post, I describe my technique for automating the creation of initial secrets for use in Ansible playbooks.\n\nMost applications need some secret state for operation. This includes passwords for database connections, random numbers used for security purposes, API keys, SSH keys, and more.\n\nSome of these secrets are user-defined, and some come from third-party or external services. User-defined secrets need to be generated at least once when deploying the application, and possibly more if a secret rotation strategy is used. For this post, I will describe a simple way to generate secrets that integrates nicely with Ansible.\n\nSome people prefer to use features built into Ansible, such as [`ansible.builtin.password`](https://docs.ansible.com/ansible/latest/collections/ansible/builtin/password_lookup.html), but I prefer my method as it integrates nicely into my vault setup and gracefully handles adding new initial secrets throughout the development process.\n\n## Technique\n\nI use a small Python program to manage the creation of secrets. I use this in combination with the Ansible Vault and an Ansible `role` to create secrets on the first run and if new secrets are created, store them in a vault, and then inject them as `facts` into the subsequent roles.\n\n**Python Program**\nThe Python program takes in a list of secrets from stdin, the name of a YAML file, and outputs a YAML file that has a secret for each one specified via the command line. The program will also preserve any secrets that are not specified, allowing manually added secrets to be stored in the same vault. \n\n\n```Python\nimport argparse\nimport random\nimport string\nimport yaml\nimport sys\n\ndef generate_secret(length=64):\n    \"\"\"Generate a random alphanumeric secret of specified length.\"\"\"\n    characters = string.ascii_letters + string.digits\n    return ''.join(random.choice(characters) for _ in range(length))\n\ndef create_secrets_yaml(identifiers, existing_secrets=None):\n    \"\"\"Create a dictionary of secrets for given identifiers.\"\"\"\n    secrets = {}\n    for identifier in identifiers:\n        if existing_secrets and identifier in existing_secrets:\n            secrets[identifier] = existing_secrets[identifier]\n        else:\n            secrets[identifier] = generate_secret()\n    return yaml.dump(secrets, default_flow_style=False)\n\ndef regenerate_secrets(secrets, identifiers_to_regenerate):\n    \"\"\"Regenerate secrets for specified identifiers.\"\"\"\n    for identifier in identifiers_to_regenerate:\n        if identifier in secrets:\n            secrets[identifier] = generate_secret()\n    return secrets\n\ndef get_changed_names(old_secrets, new_secrets):\n    \"\"\"Get the list of added, removed, and changed secret names.\"\"\"\n    added = set(new_secrets.keys()) - set(old_secrets.keys())\n    removed = set(old_secrets.keys()) - set(new_secrets.keys())\n    changed = {k for k in old_secrets.keys() & new_secrets.keys() if old_secrets[k] != new_secrets[k]}\n    return added, removed, changed\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Generate, edit, add, or remove secrets for given identifiers.\")\n    parser.add_argument('identifiers', nargs='*', help='List of identifiers to generate secrets for')\n    parser.add_argument('--regenerate', nargs='*', help='List of identifiers to regenerate secrets for')\n    parser.add_argument('--changed-names', action='store_true', help='Output what keys were added, removed or changed')\n    args = parser.parse_args()\n\n    # Read existing secrets from stdin if provided\n    existing_secrets = {}\n    if not sys.stdin.isatty():\n        existing_secrets = yaml.safe_load(sys.stdin)\n\n    old_secrets = existing_secrets.copy()\n\n    # Regenerate specified secrets\n    if args.regenerate:\n        existing_secrets = regenerate_secrets(existing_secrets, args.regenerate)\n\n    # Generate new secrets\n    new_secrets = yaml.safe_load(create_secrets_yaml(args.identifiers, existing_secrets))\n\n    # Output changed names if requested\n    if args.changed_names:\n        added, removed, changed = get_changed_names(old_secrets, new_secrets)\n        for name in added:\n            print(f\"+ {name}\")\n        for name in removed:\n            print(f\"- {name}\")\n        for name in changed:\n            print(f\"~ {name}\")\n    else:\n        # Output the YAML\n        print(yaml.dump(new_secrets, default_flow_style=False))\n\n```\n\n\nI created the following Ansible `role` to generate the initial secrets:\n\n```YAML\n---\n- name: Check if secrets file already exists\n  stat:\n    path: \"{{ playbook_dir }}/secrets/secrets.yml\"\n  register: secrets_file\n\n- name: Ensure secrets directory exists\n  file:\n    path: \"{{ playbook_dir }}/secrets\"\n    state: directory\n    mode: '0700'\n\n- name: Generate or update secrets\n  block:\n    - name: View existing secrets\n      ansible.builtin.shell: ansible-vault view {{ playbook_dir }}/secrets/secrets.yml\n      register: existing_secrets\n      when: secrets_file.stat.exists\n\n    - name: Check for changes in secrets\n      ansible.builtin.shell: >\n        {% if secrets_file.stat.exists %}\n        echo \"{{ existing_secrets.stdout }}\" | \n        {% endif %}\n        python3 {{ role_path }}/files/manage_secrets.py {{ initial_secrets | join(' ') }} --changed-names\n      register: secrets_changes\n\n    - name: Generate or update secrets using manage_secrets.py\n      ansible.builtin.shell: >\n        {% if secrets_file.stat.exists %}\n        echo \"{{ existing_secrets.stdout }}\" | \n        {% endif %}\n        python3 {{ role_path }}/files/manage_secrets.py {{ initial_secrets | join(' ') }}\n      register: secrets_output\n      when: secrets_changes.stdout != \"\"\n\n    - name: Create temporary secrets file\n      copy:\n        content: \"{{ secrets_output.stdout }}\"\n        dest: \"{{ playbook_dir }}/secrets/temp_secrets.yml\"\n        mode: '0600'\n      when: secrets_changes.stdout != \"\"\n\n    - name: Encrypt secrets file with Ansible Vault\n      command: >\n        ansible-vault encrypt \n        {{ playbook_dir }}/secrets/temp_secrets.yml\n      register: encrypt_result\n      when: secrets_changes.stdout != \"\"\n\n    - name: Rename encrypted secrets file\n      command: mv {{ playbook_dir }}/secrets/temp_secrets.yml {{ playbook_dir }}/secrets/secrets.yml\n      when: secrets_changes.stdout != \"\" and encrypt_result.rc == 0\n\n    - name: Clean up temporary files\n      file:\n        path: \"{{ playbook_dir }}/secrets/temp_secrets.yml\"\n        state: absent\n      when: secrets_changes.stdout != \"\" and encrypt_result.rc != 0\n\n    - name: Display success message\n      debug:\n        msg: \"Secrets file {% if secrets_file.stat.exists %}updated{% else %}created{% endif %} and encrypted successfully at {{ playbook_dir }}/secrets/secrets.yml\"\n      when: secrets_changes.stdout != \"\"\n\n    - name: Display no changes message\n      debug:\n        msg: \"No changes detected in secrets file. Skipping re-encryption.\"\n      when: secrets_changes.stdout == \"\"\n```\n\n\nWith these two components in place, all that is left to do is load the secrets into [`facts`](https://www.redhat.com/sysadmin/playing-ansible-facts), and use them in other roles in my playbook.\n\n```YAML\n- name: Web Servers\n  hosts: web:!localhost\n  tags: [remote, staging]\n  tasks:\n    - name: Include the encrypted secrets file as variables\n      ansible.builtin.include_vars:\n        file: \"inventories/{{ env }}/group_vars/secrets.yaml\"\n      tags: [always]\n\n    - name: Setup Web servers\n      ansible.builtin.include_role:\n        name: deploy_site\n```"
    },
    "/blog/2024-09-02-personal-microservices-infrastructure-project": {
        "title": "Personal Microservices Infrastructure Project",
        "date": "2024-09-02",
        "tags": [
            "devops",
            "programming",
            "kubernetes",
            "hashicorp",
            "terraform",
            "homelab"
        ],
        "publish": true,
        "url": "/blog/2024-09-02-personal-microservices-infrastructure-project",
        "path": "blog/01-personal-infrastructure.md",
        "content": "![Image of people building their own digital infrastructure](/images/personal-cloud.png)\n\nJoin me as I build a personal datacenter, a \u201chomelab\u201d suitable for playing with microservices infrastructure. This is the first chapter of a story where people reclaim their power from their digital lords, and grow into equal peers in the digital realm.\n\nI am starting my portion of this journey by arranging computer and software components to build a datacenter to serve as the foundation of a \"Personal Cloud.\" This personal cloud will create my own digital sovereign territory from which I can steward my information and define my interaction with my peers. This personal cloud is intended as a proof of concept of what a better internet could look like.\n\n### Introduction\n\nThere was a time when only a few wealthy people had books. There was a time when only a few people had telephones. There was a time when only a few people had computers. We are now in an era where many people have easy access to these modern information technologies. In this era, there are only a few people who have their own datacenter. Everything is in \u201cthe cloud\u201d, a new form of digital feudalism reigns supreme. Most people life as serfs, graciously accepting the few crumbs of value that trickle down in the form of apps and services while those people controlling them accumulate wealth and power.\n\n## The Nitty Gritty\n\nThis project aims to create a comprehensive, real-world microservices architecture, focusing on automation, scalability, and best practices in modern DevOps.\n\nI'll write developer journal entries as blog posts, as well as organize them into a more structured \"how-to\" documentation suitable for recreating the project or following along. I\u2019ll also write the occasional essay about the meaning and \u2018philosophy\u2019 motivating my actions.\u00a0\n\nAs of now, I've done some preliminary research and have come up with this plan of action. As I go ahead and my hands dirty, I imagine some of this plan will change, either slightly or dramatically as I learn from my mistakes and refine my understanding.\n\n### Tentative Plan of Action\n\n#### 1. Physical Server Setup\n  - [[blog/01-ansible-setup]]\n  - [[blog/02-ansible-secrets]]\n  - [[blog/03-simple-automation]]\n  - [[blog/04-initial-secrets]]\n\n#### 2. CI Runner Setup:\n   - Install and configure a CI tool (likely [Drone CI](https://www.drone.io/))\n   - Migrate legacy services to Ansible, deployed by CI\n   - Automatically run Ansible on infrastructure code changes.\n\n#### 3. Core Services\nIn this phase, I will deploy a small cluster of virtual machines that will host \u201ccore services\u201d that will make deploying production services easier and more repeatable.\n\n- Use [Packer](https://www.packer.io/) for creating standardized VM images\n- Implement [Terraform](https://www.terraform.io/) for infrastructure provisioning\n- Setup [Headscale](https://www.headscale.net/) for secure networking\n- Set up [Nomad](https://www.nomadproject.io/) for initial workload orchestration\n- Deploy [Vault](https://www.vaultproject.io/) for secrets management\n- Set up monitoring tools (e.g., [Prometheus](https://prometheus.io/), [Grafana](https://grafana.com/))\n- Implement logging solution (e.g., [ELK Stack](https://www.elastic.co/elastic-stack))\n\n#### 4. Advanced Orchestration:\n   - Create a separate Nomad cluster for application workloads\n   - Potentially set up [K3s](https://k3s.io/) (lightweight Kubernetes) within Nomad\n\n#### 5. Network and Security:\n   - Implement ingress controllers and load balancers\n   - Set up network policies and firewalls\n\n#### 6. Automation and Scalability:\n   - Develop scripts and workflows for rapid cluster creation\n   - Implement auto-scaling and self-healing capabilities\n#### 7. Begin deploying Services\n\nIn this phase, I will begin with the working foundation to deploy modern applications that I\u2019ve build in subsequent steps. From here I will begin to make this cloud more personal, and develop applications and services on top of it that are valuable to me, and demonstrate the value of a personal cloud.\n\n### Learning Objectives\n\nWhy am I doing this? I'd like to learn these tools, and build something useful for myself and others. I have a large project in mind, and I will discuss it after I build the initial proof of concept \"personal cloud\".\n\nPut into dry bullet points, I'd like to:\n\n- Gain hands-on experience with modern DevOps tools and practices\n- Understand whether microservices architecture is a good fit for my project\n- Develop skills in automation, security, and scalability in distributed systems\n- Build something useful for myself and others\n\n### Sharing Knowledge\n\nThroughout this project, I will:\n\n  - Document each step in this developer journal or blog\n  - Create some guides and tutorials\n  - Share challenges faced and solutions implemented. I'd like to specifically share my perspective, from someone new to \n    modern DevOps, though no stranger to software engineering.\n  - Possibly create video content"
    },
    "/blog/2024-09-27-how-to-solve-any-problem-and-win": {
        "title": "How to Solve Any Problem and Win",
        "date": "2024-09-27",
        "tags": [
            "blog",
            "mental-models"
        ],
        "publish": true,
        "url": "/blog/2024-09-27-how-to-solve-any-problem-and-win",
        "path": "blog/12-how-to-solve-any-problem-and-win.md",
        "content": "![Beautiful picture of swirling spirals in space with planets, galaxies and glowing colors](/images/mental-models-1.webp)\n\nIn this article I describe in broad strokes how I approach any problem and succeed.\n\nSupport me by subscribing on [Substack](https://substack.com/home/post/p-149521497?source=queue&autoPlay=false)\n\nTo demonstrate that such a bold claim is true, I'll start with a few definitions to make sure we're on the same page. These are my definitions for this article, and I advise against thinking of them as any universal truth. \n\n1. **Problem:** An obstacle preventing the realization of a mentally conceived state or experience.  \n2. **Success:** The achievement or realization of a mentally conceived state or experience. \n\nGiven these definitions, a problem presupposes the intention to achieve a mentally conceived state or experience. It follows that to solve a problem, there most first be an intended state, and there must be an obstacle preventing it.\n\nGiven this, the first step to solve any problem is to ask yourself: Is this really a problem? For it to be a problem, you must first have a conception of what success \"looks like.\" Otherwise, you will never be able to determine if you achieved it. \n\nOnce you know what the desired state or experience is, ask yourself: What is preventing the achievement of this state or experience?\n\nOnce these two questions are answered, solving the problem flows naturally. \n\n I'll add that some problems may take so long to solve that they are effectively unsolvable. In this case, this process may not be of much use to anyone.\n\n## Concrete Example\n\nThese definitions are rather abstract. As my writings this far have focused on highly technical aspects of computer programming, I'll choose a concrete examples and explore it.\n\nCurrently I'm in the process of building a Personal Cloud or as I've named it earlier Personal Infrastructure.  My architecture as outlined in [[blog/08-support-services-overview]] calls for running virtual machines on physical servers. I've chosen to try to use [libvirt](https://libvirt.org), specifically using the [kvm](https://linux-kvm.org/page/Main_Page) virtualization technology. I'd like to use [Terraform](https://www.terraform.io) or [Pulumi](https://www.pulumi.com) to quickly spin up VMs on one of my several physical machines by running a single command, or possibly just by committing code to a repository. Although not fully and rigorously specified in language, success looks a bit like this:\n\n```bash\n$ kubectl get nodes\nNAME           STATUS   ROLES    AGE     VERSION\nworker-node1   Ready    <none>   7d2h    v1.22.5\nworker-node2   Ready    <none>   7d1h    v1.22.5\nmaster-node    Ready    master   7d3h    v1.22.5\n\n$ git commit -m \"Add another VM for K8s cluster\"\n[master 3a1b2c3] Add another VM for K8s cluster\n 1 file changed, 10 insertions(+), 2 deletions(-)\n\n$ git push\nEnumerating objects: 5, done.\nCounting objects: 100% (5/5), done.\nDelta compression using up to 8 threads\nCompressing objects: 100% (3/3), done.\nWriting objects: 100% (3/3), 354 bytes | 354.00 KiB/s, done.\nTotal 3 (delta 2), reused 0 (delta 0)\nTo github.com:yourusername/your-repo.git\n   abc1234..3a1b2c3  master -> master\n\n$ sleep 30\n\n$ kubectl get nodes\nNAME           STATUS   ROLES    AGE     VERSION\nworker-node1   Ready    <none>   7d2h    v1.22.5\nworker-node2   Ready    <none>   7d1h    v1.22.5\nworker-node3   Ready    <none>   2m      v1.22.5\nmaster-node    Ready    master   7d3h    v1.22.5\n```\n\n\nThis imaginary transcript shows:\n\n1) The beginning state: Three computers available in a [kubernetes](https://kubernetes.io)\n2) The implied changing of some file or files\n3) Pushing these changes to some server somewhere\n4) Four computers available\n\nOnce this becomes realized, I'll have the delightful experience of being able to manifest virtual machines with a few keystrokes, a resounding success.\n\n\nHowever, I have a problem: It doesn't work! I haven't even attempted anything with [kubernetes](https://kubernetes.io), but I know there is a problem because when I run\n\n```bash\n$ pulumi up\n```\n\nI get lots of errors! So now what?\n\n\n## Problem Solving Strategy\n\nFirst, when I appear to have encountered a problem, I ask myself \"Is this really a problem?\" Using the above words as guidance, for there to be a problem, there must be a intended mental conceived state or experience, and obstacle preventing its actualization. If there is no clear desired state, maybe there isn't a problem? \n\nHaving established that there is a problem, I divide problem solving into three main methods:\n\n1. **Intuitive Exploration**: This method relies on a deep, subconscious understanding of the problem domain. In this method, one follows \"gut feelings\" or intuitions about potential solutions as the primary driver. While appearing random to an outside observer, these attempts are guided by an internal mental model that operates as a \"black box\" from the perspective of conscious reasoning.\n\n2. **Minimalist Learning**: This approach involves acquiring only the essential knowledge required to solve the problem at hand. One attempts to build smallest mental model needed, followed by taking the minimum number of steps to solve the problem.\n\n3. **Comprehensive Modeling**: This method aims to construct a \"perfect\" mental model of the problem space before taking action. Once this model is established, the solution becomes apparent, allowing for the implementation of the bare minimum actions needed to solve the problem.\n\nThese methodologies exist on a continuum, and their effectiveness varies depending on the nature of the problem and the one's expertise. In practice, a combination of these approaches often yields the most efficient path to resolution.\n\nMy personal algorithm for problem-solving typically follows this pattern:\n\n1. **Initial Intuition**: If I have a strong intuition about a potential solution, I attempt it immediately and reassess based on the outcome.\n\n2. **Reassessment**: If the intuitive approach proves unfruitful, I transition to expanding my mental model.\n\n3. **Information Gathering**: This stage involves researching error messages, consulting various sources (including Large Language Models), and examining adjacent problems and solutions.\n\n4. **Rigorous Modeling**: If the problem persists, I engage in comprehensive mental model building. This involves documenting all known information, identifying knowledge gaps, and engaging in deliberate research to fill these gaps. I may actualize my mental models using code, diagrams, or other tools.\n\n5. **Iterative Refinement**: As the mental model evolves, I may return to intuitive attempts or conduct careful experiments to further refine my understanding and approach.\n\n6. **Systematic Experimentation**: With a more complete mental model, I design and execute precise experiments to either solve the problem directly or gain deeper insights into the system's behavior.\n\nAn interesting aside is that this mental model can be applied to itself. Consider this: success can be defined as the ability to manifest any mental conception. In this context, the problem becomes not having a sufficient mental model to achieve something in the physical world. One can apply the ideas in this article to further refine the mental model. This process doesn't stop there. It's possible to then refine the mental model used to refine the initial mental model. This creates an infinite recursive spiral, potentially leading to enlightenment\u2014or possibly insanity.\n\n## Problem Solving Diagram\n\n```mermaid\ngraph TD\n    A[Problem Identified] --> B{Intuitive Solution?}\n    B -->|Yes| C[Attempt Intuitive Solution]\n    C --> D{Problem Solved?}\n    D -->|Yes| E[End]\n    D -->|No| F[Reassess]\n    B -->|No| F\n    F --> G[Expand Mental Model]\n    G --> H{Sufficient Understanding?}\n    H -->|Yes| I[Design Experiment]\n    H -->|No| J[Rigorous Modeling]\n    J --> K[Document Known Information]\n    K --> L[Identify Knowledge Gaps]\n    L --> M[Conduct Research]\n    M --> N{Model Complete?}\n    N -->|Yes| I\n    N -->|No| J\n    I --> O[Execute Experiment]\n    O --> P{New Insights Gained?}\n    P -->|Yes| G\n    P -->|No| Q{Problem Solved?}\n    Q -->|Yes| E\n    Q -->|No| R{Continue?}\n    R -->|Yes| F\n    R -->|No| S[Meditate until there is no proglem]\n\n```\n\n\n\n## What's Wrong with libvirt?\n\nI've been applying my problem-solving strategy to the issue I described earlier. I probably won't write about all of it, but here are the concrete steps I've taken so far:\n\n1. Learn a bit about Pulumi.\n2. Create a quick and dirty `pulumi` project with the help of [Cursor](https://cursor.com).\n3. Try to run it.\n4. Encounter SSH permission issues.\n\nThis was the first clear obstacle preventing progress. I paused, then leaned into my intuition to start trying various things to debug SSH. I tried connecting manually with my user account. I tried the `ansible_user` account I was also using for Pulumi. When I encountered a failure, I started looking at my SSH keys, my known hosts file, and the `authorized_keys` file on the remote host. \n\nAfter about 10 minutes, I realized the problem was actually that my `.ssh/config` file contained an entry for `aslan` with a username of `igutek` and a specified private key. From this, I tried adding a new entry for `ansible_user` with the correct private key specified, and then all was well.\n\nThis was a problem that was solved while staying almost entirely within my intuition. I then started plowing headfirst into Pulumi errors because I was using it wrong. The error messages made it clear, and I tried a few things \"intuitively\" before turning to the documentation with a bit of LLM assistance. I didn't read all the documentation, just the bare minimum to make forward progress. \n\nWith time, I moved past this and started seeing problems about insufficient permissions to do certain actions related to virtual machines. After 30 minutes of intuition and gentle expansion of my mental model through web searches, I moved on to rigorous modeling. For the rest of my working hours today \u2014 with the exception of time taken to write this article \u2014 I've been reading and learning about [libvirt](https://libvirt.org).\n\n## Bonus Material\n\nI've hesitated a bit about sharing this, because maybe I'm worried about being judged for being stupid. However, I've decided to post it anyway.\n\n\n![Screenshot of Obsidian note showing messy work in progress notes for understanding libvirt](/images/virtio-problem-solving.png)\n\nA few things I'd like to call your attention to:\n\n1. I'm using Obsidian and Markdown.\n2. This is a draft for a future article on [isaac.cc](https://isaac.cc).\n3. When I started it, I did not know some words that might be embarrassingly obvious to some people, like **paravirtualization**.\n4. My notes are mostly a scattered list of links and look chaotic. This is because I'm storing the minimal amount of information to proceed with my mental model building, most of which is... well mental. Should I decide to render some of these thoughts into text form, I will obviously clean it up."
    }
}