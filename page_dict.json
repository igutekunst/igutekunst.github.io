{
    "/nav": {
        "title": "nav",
        "date": "2024-09-20",
        "tags": [],
        "publish": true,
        "url": "/nav",
        "path": "nav.md",
        "content": "# Example Site\n\n## Navigation\n- [Blog](blog)\n- [About](about)\n- [Contact](contact)"
    },
    "/contact": {
        "title": "Contact Me",
        "date": "2024-09-20",
        "tags": [],
        "publish": true,
        "url": "/contact",
        "path": "contact.md",
        "content": "# Connect With Me\n\nYou can connect with me in a few ways:\n\n - Connect on [Matrix](https://matrix.org) [@isaac:iamthatiam.org](https://matrix.to/#/@isaac:iamthatiam.org)\n - Comment on this or any page"
    },
    "/./2024-09-17-deploying-django-to-digital-ocean": {
        "title": "Deploying Django to Digital Ocean",
        "date": "2024-09-17",
        "tags": [
            "devops",
            "django"
        ],
        "publish": true,
        "url": "/./2024-09-17-deploying-django-to-digital-ocean",
        "path": "django-deploy-notes.md",
        "content": "Although not strictly part of the [[blog/01-personal-infrastructure]] series, this blog post will discuss building and deploying a basic Django application.\n\n\nSecrets:\n\n- Postgres\n- Django admin\n- Django token\n- SSH Keys\n- Ansible Vault Password\n\nAnsible\n\n2 Docker Images\n\n- Nginx\n- Django\n\nQuestions:\n\nShould .env file be backed into container? No\n - Can be loaded by  `compose.yaml`\n - Possibly store in ansible vault, then copied to production box\n\nWhat server deploys to production\nProbably a DroneCI runner. It will need an SSH deploy key to prod DO box.\n\nDigital Ocean Load Balancer\nDO Box, with docker compose situation. This has nginx, postgres, celery etc.\n\nShould I pay DO for a managed database? Probably\n\nWhat does local development look like?\n\nThere will be a a few environments: \n- Fully local with sqlite\n- Local in compose (possibly should add a DNS situation and run through `gateway`)\n-"
    },
    "/": {
        "title": "Programming Reality",
        "date": "2024-08-31",
        "tags": [],
        "publish": true,
        "url": "/",
        "path": "index.md",
        "content": "Welcome to Programming Reality, a site about changing the nature of reality, one line of code at a time.\n\n\n\n## Featured Posts {: .featured-posts}\n - [[blog/03-simple-automation]]{: .featured-post}\n - [[blog/01-personal-infrastructure]]{: .featured-post}\n - [[blog/00-welcome]]{: .featured-post}"
    },
    "/about": {
        "title": "About",
        "date": "2024-09-20",
        "tags": [],
        "publish": true,
        "url": "/about",
        "path": "about.md",
        "content": "# About the Site\nThis is a personal website for Isaac Harrison Gutekunst. I've created this site to share my exploration of programming reality. The main focus of this is highly technical articles about computer programming, engineering, and [[philosophy]]. \n\n## Content\n\nThe site is structured primarily as a \"blog\", where I write about projects I am working on, or small pieces of knowledge I've acquired over time. \n\n## Technical Aspects of the Site\n\nThis site is hosted on Github Pages, and generated using a custom static site generator written in Python. It uses a customized [Bootstrap](https://getbootstrap.com) for CSS. It generates the entire site from a collection of markdown files, with the YAML [`frontmatter`](https://dpericich.medium.com/what-is-front-matter-and-how-is-it-used-to-create-dynamic-webpages-9d8dc053b457) and custom syntax being inflated into metadata and content on the site.\n\nI am aware the colors, typography, and layout can be improved. I decided it was more important to get the site out than to get it perfect. If you'd like to help with CSS, don't hesitate to [[contact|contact]] me."
    },
    "/blog/2024-08-27-welcome-to-programming-reality": {
        "title": "Welcome to Programming Reality",
        "date": "2024-08-27",
        "tags": [
            "blog",
            "philosophy",
            "devops"
        ],
        "publish": true,
        "url": "/blog/2024-08-27-welcome-to-programming-reality",
        "path": "blog/00-welcome.md",
        "content": "[Read on Substack](https://programmingreality.substack.com/p/welcome-to-programming-reality)\n\n[Read on LinkedIn](https://www.linkedin.com/pulse/welcome-programming-reality-isaac-gutekunst-a02ic/)\n\nWelcome to my website and blog. Join me as I share my exploration of reality through the lens of language, and primarily machine-readable language.\n\nI will group my writing into three or more primary categories:\n\n1) My journey working on long-term projects. Posts in this category will be highly technical, and involve topics like computer programming, distributed systems, databases, networking, protocols and more.\n\n2) General musings on the \"Nature of Reality\". I'll typically tag these as \"philosophy\".\n\n3) Miscellaneous things I'm learning. I'll tag these as \"did-you-know\".\n\nI will kick off this journey by introducing my first long-term project: Building a \"Personal Cloud\", using Infrastructure as Code (IaC) tools and my best attempt at building something production-ready.\n\n## Programming Reality\n\nProgramming Reality as phrase is intended to evoke the idea that reality is flexible, and can be programmed with intention. People have always programmed reality with their words by telling stories. The stories we tell each other shape how we see the world, and perhaps even shape the structure of reality itself. In the current age, the programming is becoming more explicit and less occulted. People write computer programs that capture intentions, and then when placed into the right environment have real measurable affects on the physical world. I can tap my fingers on a piece of glass in a certain pattern and  30 minutes later, someone will bring food to my door. This is the magic of Uber Eats on an iPhone using the Internet.\n\n\n# About Me\nIn my professional career, I've spent over a decade working to build various computer systems, primarily for robotics, consumer electronics and aerospace. I've written code that is likely running in mars, running on a satellite orbiting overhead, running inside the headphones worn by millions of people, and running in many more glamorous and not so glamorous environments.\n\nPersonally, I've always been drawn to look closely at the inner workings of everything. I am drawn to understand how light bulbs function, and also why people go to war. I love looking at the structure of companies, families and societies big and small. I see patterns repeating at every level of abstraction, from the organization of ants, to the arrangement of code within a distributed system. \n\n## Why I'm Publishing this Site\n\nI love the world, and feel like sharing. I like building cool stuff! I think everything about life is amazing, and want to share that!\n\nI have so many ideas in my head that I love and want to see take physical form. I've decided to take the next step and start writing about them publicly."
    },
    "/blog/2024-09-15-personal-infrastructure-part-3:-quality-of-life-improvements-with-justfile-automation": {
        "title": "Personal Infrastructure Part 3: Quality of Life Improvements with Justfile Automation",
        "date": "2024-09-15",
        "tags": [
            "blog",
            "development",
            "infrastructure",
            "ansible"
        ],
        "publish": true,
        "url": "/blog/2024-09-15-personal-infrastructure-part-3:-quality-of-life-improvements-with-justfile-automation",
        "path": "blog/03-simple-automation.md",
        "content": "![Image of high tech secure vault](/images/ansible-vault-automation-1.webp)\n\n\nIn this post, I describe how I like to use Justfiles to make running common tasks easier.\n\n## 1. Justfile:\n\nI created a Justfile to streamline the setup and execution of various tasks in our project. \n\nJustfiles are simplified modern alternatives to Makefiles. Here's a quick overview of what our Justfile does:\n\nI prefer them for simple task execution that doesn't require the complexity and dependency resolution of Makefiles. I also like avoiding Makefile syntax whenever possible.\n\n\nThe `Justfile` has recipes for the following:\n\n- Creating and activating a virtual environment.\n- Installing or verifying dependencies (Ansible and Cookiecutter).\n- Generating a vault password for Ansible.\n- Creating secrets for our infrastructure.\n- Setting up the entire environment in one go.\n- Running the Ansible playbook.\n\nThe Justfile simplifies our workflow by encapsulating complex commands into simple, memorable recipes. For example, instead of remembering long command sequences, we can now just run `just setup` to prepare our environment or `just ansible_playbook` to execute our Ansible playbook.\n\nThis approach not only saves time but also reduces the likelihood of errors, ensuring consistency across different development environments and making it easier for team members to contribute to the project.\n\n\n```Justfile\n# Justfile\n\n# Set the shell to bash\nset shell := [\"bash\", \"-cu\"]\n\n# Define a variable for the virtual environment directory\nvenv_dir := \"venv\"\n\n# Recipe to create or activate the virtual environment\nvenv:\n    if [ ! -d {{venv_dir}} ]; then \\\n        python3 -m venv {{venv_dir}}; \\\n    fi\n    . {{venv_dir}}/bin/activate\n\n# Recipe to install or verify Ansible and Cookiecutter are installed\ninstall_dependencies:\n    just venv\n    . {{venv_dir}}/bin/activate\n    if ! pip show ansible > /dev/null 2>&1; then \\\n        pip install ansible; \\\n    else \\\n        echo \"Ansible is already installed\"; \\\n    fi\n    if ! pip show cookiecutter > /dev/null 2>&1; then \\\n        pip install cookiecutter; \\\n    else \\\n        echo \"Cookiecutter is already installed\"; \\\n    fi\n\ncreate_vault_password:\n    python3 physical-server-ansible-playbook/get_vault_pass.py generate\n\ncreate_secrets:\n    python3 physical-server-ansible-playbook/create_secrets.py cir\n\n# Recipe to set up the environment (create venv and install dependencies)\nsetup:\n    just venv\n    just install_dependencies\n    just create_vault_password\n\n# Add more tasks as needed\n\n\nansible_playbook:\n    just setup\n    cd physical-server-ansible-playbook && ansible-playbook playbook.yml\n\nping:\n    just setup\n    cd physical-server-ansible-playbook && ansible -i inventory/hosts all -m ping\n\n```\n\n### 1.1 Makefile\n\nTo really push the automation to the next level, I made a Makefile that install [just](https://github.com/casey/just).\n\n\n```bash\n# Install Just\n\n# Detect the operating system\nUNAME_S := $(shell uname -s)\n\n# Default installation method (for unsupported systems)\ninstall_just_default:\n\t@echo \"Unsupported operating system. Please install Just manually.\"\n\n# macOS installation\nifeq ($(UNAME_S),Darwin)\ninstall_just:\n\t@if command -v port >/dev/null 2>&1; then \\\n\t\tsudo port install just; \\\n\telif command -v brew >/dev/null 2>&1; then \\\n\t\tbrew install just; \\\n\telse \\\n\t\techo \"Neither MacPorts nor Homebrew is installed. Installing Homebrew...\"; \\\n\t\t/bin/bash -c \"$$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"; \\\n\t\tbrew install just; \\\n\tfi\nelse\n\n# Debian-based Linux installation\nifeq ($(UNAME_S),Linux)\ninstall_just:\n\t@if command -v apt-get >/dev/null 2>&1; then \\\n\t\tsudo apt-get update && sudo apt-get install -y curl; \\\n\t\tcurl --proto '=https' --tlsv1.2 -sSf https://just.systems/install.sh | sudo bash -s -- --to /usr/local/bin; \\\n\telse \\\n\t\techo \"This doesn't appear to be a Debian-based system. Please install Just manually.\"; \\\n\tfi\nelse\n\n# Fallback to default installation method\ninstall_just: install_just_default\n\nendif\nendif\n\n.PHONY: install_just install_just_default\n\n```\n\n\n## Next Steps: Creating initial secrets automatically\n\nAfter I can store values securely, I'd like to automate the creation of initial random secrets used for various services.\n\nRead more about it in my post: [[blog/04-initial-secrets]]"
    },
    "/blog/2024-09-14-personal-infrastructure-part-2:-setting-up-secret-storage-for-ansible": {
        "title": "Personal Infrastructure Part 2: Setting up Secret Storage for Ansible",
        "date": "2024-09-14",
        "tags": [
            "blog",
            "development",
            "infrastructure",
            "ansible"
        ],
        "publish": true,
        "url": "/blog/2024-09-14-personal-infrastructure-part-2:-setting-up-secret-storage-for-ansible",
        "path": "blog/02-ansible-secrets.md",
        "content": "![Image of high tech secure vault](/images/ansible-vault-1.webp)\n\n\nIn this post, I'm going to explain one way to store secrets when using Ansible.\n\nAnsible has the ability to encrypt and decrypt data, using what it calls the [Ansible Vault](https://docs.ansible.com/ansible/latest/vault_guide/index.html).\n\n## Introduction\n\nMany services require passwords, keys and other secrets. Some are used to access systems and services outside of the ansible deployment, and many are often randomly generated during the initial setup for use within the deployment.\n\nIn both cases, I like encrypting these using Ansible Vault. To make it a bit smoother, I take advantage of a few Ansible features.\nAfter digging around, and doing this a few times, I've settled on the following technique:\n\n1. I use a Python script to retrieve the key used by Ansible Vault to encrypt and decrypt. \n2. Edit `ansible.cfg` to use this script\n3. Make another python script and Ansible Playbook that create new random secrets for usage within playbooks.\n\n\n## Pieces\n\n### 1. Store Vault Password\n\nI made a simple python script that for storing a password in the system's protected storage. This should work on Windows, MacOS and Linux (in Desktop mode), though I haven't tested on anything except MacOS:\n\n```Python\n#!/usr/bin/env python\n\nimport os\nimport sys\nimport keyring\nimport getpass\nimport argparse\nimport secrets\nimport string\n\nAPP_ENV = os.getenv(\"APP_ENV\",\"development\")\n\nSERVICE_NAME = \"AnsibleVault\"\nACCOUNT_NAME = f\"ansible_vault_password_rev_{APP_ENV}\"\n\ndef get_vault_password():\n    password = keyring.get_password(SERVICE_NAME, ACCOUNT_NAME)\n    return password\n\ndef set_vault_password(generate=False):\n    if generate:\n        password = ''.join(secrets.choice(string.ascii_letters + string.digits + string.punctuation) for _ in range(32))\n        print(\"Generated a new secure password.\")\n    else:\n        password = getpass.getpass(\"Enter New Ansible Vault password: \")\n    keyring.set_password(SERVICE_NAME, ACCOUNT_NAME, password)\n    return password\n\ndef clear_vault_password():\n    keyring.delete_password(SERVICE_NAME, ACCOUNT_NAME)\n    print(\"Ansible Vault password has been cleared.\")\n\nif __name__ == \"__main__\":\n    if len(sys.argv) == 1:\n        password = get_vault_password()\n        if not password:\n            sys.stderr.write(\"No Ansible Vault password found. Please set or generate one.\")\n            sys.exit(1)\n        print(password)\n    else:\n        parser = argparse.ArgumentParser(description=\"Manage Ansible Vault password\")\n        parser.add_argument(\"action\", choices=[\"set\", \"generate\", \"clear\"], help=\"Action to perform\")\n        args = parser.parse_args()\n\n        if args.action == \"set\":\n            set_vault_password()\n            print(\"Ansible Vault password has been set.\")\n        elif args.action == \"generate\":\n            stored_password = keyring.get_password(SERVICE_NAME, ACCOUNT_NAME)\n            if stored_password:\n                print(\"Ansible Vault password already exists. Use set to set it, or clear to clear it.\")\n            else:\n                set_vault_password(generate=True)\n                print(\"Ansible Vault password has been set.\")\n        elif args.action == \"clear\":\n            clear_vault_password()\n```\n\n### Configure Ansible to use Python Script\n\nI edited `ansible.cfg` to use the new python script:\n\n```bash\ncat ansible.cfg                                                                                                                                                                                                                master \u2b06 \u2716 \u25fc\n[defaults]\ninventory = inventory/hosts\nremote_user = ansible_user\nprivate_key_file = ~/.ssh/id_ed25519_aslan_ansible\nhost_key_checking = False\n+++ vault_password_file = get_vault_pass.py\ninterpreter_python = auto_silent\n```\n\n**Important:**  Be sure to make sure get_vault_pass.py is executable and has an appropriate `#!/usr/bin/env python` or similar.\n\n\n### Create initial vault password\n\n```bash\n    APP_ENV=staging python3 physical-server-ansible-playbook/get_vault_pass.py generate\n    APP_ENV=development python3 physical-server-ansible-playbook/get_vault_pass.py generate\n    APP_ENV=production python3 physical-server-ansible-playbook/get_vault_pass.py generate\n    \n```"
    },
    "/blog/2024-09-13-personal-infrastructure-part-1:-introduction-and-basic-ansible-setup": {
        "title": "Personal Infrastructure Part 1: Introduction and Basic Ansible Setup",
        "date": "2024-09-13",
        "tags": [
            "blog",
            "development",
            "infrastructure"
        ],
        "publish": true,
        "url": "/blog/2024-09-13-personal-infrastructure-part-1:-introduction-and-basic-ansible-setup",
        "path": "blog/01-ansible-setup.md",
        "content": "![Image of computers and power line infrastructure](images/ansible-1.webp)\n\n\nIn this first step, I'm going to build Ansible skeleton project and test connectivity.  I will explain the motivation, my existing setup, how I setup Ansible, and how I make the process a bit smoother and more secure.\n\n# Introduction and Motivation\n\nEventually I will build out [[blog/01-personal-infrastructure]]. I will have a collection of files that given access to a handful of physical or virtual machines will \"build\" a complete foundation for a personal microservices project. To avoid moving too slowly, I will avoid trying to make any part of this process perfectly generic. It will work with my chosen hardware, software and 3rd party services. I will make some effort so that anyone following alone should be able to recreate something similar. \n\n## My Existing Setup\n\n### Hardware\n\n- [Protectli Vault](https://protectli.com/vault-6-port/), configured with 64GB RAM, and a Samsung 4TB SSD.\n- AMD Threadripper desktop with 128GB RAM, and 8TB of SSD storage.\n- Old MSI laptop with 16GB or RAM and 1 TB of SSD Storage\n- Digital Ocean Intel SSD VM with 4GB of RAM and 100 GB or storage.\n- Smaller Protectli Vault running pfSense\n\n### Existing Use Cases\n\n Right now I host a few personal web services:\n\n - [Plex](plex.tv)\n - My \"spiritual\" website: [i am that i am](https://iamthatiam.org), an \"almost\" static site that uses a bit of Django.\n - A personal [Sentry](sentry.io) instance.\n - A personal [Jenkins](jenkins.io) instance\n - A [Plausible Analytics](https://plausible.io) instance\n - A few more\n\n\n### Network Topology\n\nAll the physical hardware is connected via a gigabit switch behind pfSense router. The pfSense router is running a [Wireguard](https://www.wireguard.com) server that I connect to with my roaming Mac laptop, and iPhone. All these servers are on the `10.0.0.0/24` subnet. All Wireguard clients are on the `10.2.0.0/24` subnet, with a specific tunnel for each one.\n\nMy VM provided by Digital Ocean is running Debian 12. It has a permanent Wireguard tunnel back to the pfSense box. For this website, I will call this box `gateway`.\n\nI'ved named all three physical compute nodes after cats:\n\n 1. [`aslan`](https://en.wikipedia.org/wiki/Aslan) - AMD Threadripper system, and main computer node\n 2. [`green-lion`](https://en.wikipedia.org/wiki/Suns_in_alchemy) - Large Protectli vault\n 3. [`bagheera`](https://en.wikipedia.org/wiki/Bagheera) - Old MSI latop\n\n Click on the links to see a bit behind each name. I must confess, I'm not fully aware of the history of `green-lion` within the field of alchemy, so I hope it doesn't mean something terrible!\n\nAccording to ChatGPT:\n\n>In essence, the Green Lion is a metaphor for transformation, representing both the destructive and creative forces in alchemy.\n\n\n These are running in containers manually deployed using [Docker Compose](https://docs.docker.com/compose/). Most of them are running on `aslan`, with some running in \"high availability\" mode, with containers running on both `aslan` and `green-lion`. An [nginx](nginx.com) reverse proxy runs on `gateway` proxying traffic and terminating SSL using [Let's Encrypt](letsencrypt.org).\n\n I'd like to leave all of these services running with close to zero downtime, while deploying new services using increasingly more advanced techniques, culminating in a platform built on top of Kubernetes.\n\n To do so, I'm going to first get some automation in place to configure and manage these physical servers. Then I'll move the manual configuration of my existing service into Ansible, and then from there will setup CI using [Drone CI](drone.io).\n\n# Installing and testing Ansible\n\n## What is Ansible. Why am I using it?\n\nI've used Ansible a few times to deploy web applications and configure servers. I don't love the giant collection of templated YAML, and yet, it provides too much value to ignore.\n\nAs described on their homepage:\n\n> Ansible is an open source IT automation engine that automates provisioning, configuration management, application deployment, orchestration, and many other IT processes. It is free to use, and the project benefits from the experience and intelligence of its thousands of contributors.\n\nIn my words, Ansible is a tool that let's you write YAML file that describe actions that should be taken on a collection of servers, including copying files, installing software and more. When structured and written well, Ansible \"Playbooks\" are [[idempotent]], and repeatable.\n\nI'm going to be using ansible primarily to manage the physical servers before any additional infrastructure is in place.\n\n\n## Requirements\n\nBefore we can use ansible, we need passwordless sudo ssh access to all nodes.\n\nRepeat this step for all physical nodes that you wish to manage with Ansible. I'm going to target `aslan` and `green-lion` initially, and then maybe move onto `gateway` and `bagheera` later.\n\n### 1. Passwordless sudo ansible_user account\n\n\n1. Create a new user named `ansible_user`:\n\n```\nsudo adduser ansible_user\n```\n\n2. Give `ansible_user` sudo access without requiring a password:\n\n```\necho \"ansible_user ALL=(ALL) NOPASSWD:ALL\" | sudo tee /etc/sudoers.d/ansible_user\n```\n\n3. Set up SSH key authentication for `ansible_user`:\n\n```\nsudo mkdir -p /home/ansible_user/.ssh\nsudo chmod 700 /home/ansible_user/.ssh\nsudo touch /home/ansible_user/.ssh/authorized_keys\nsudo chmod 600 /home/ansible_user/.ssh/authorized_keys\n```\n\n4. Copy your public SSH key into the `authorized_keys` file:\n\n```\nsudo sh -c 'echo \"YOUR_PUBLIC_SSH_KEY\" >> /home/ansible_user/.ssh/authorized_keys'\n```\n\n   Replace `YOUR_PUBLIC_SSH_KEY` with your actual public SSH key.\n\n5. Set proper ownership for the `.ssh` directory and its contents:\n\n```\nsudo chown -R ansible_user:ansible_user /home/ansible_user/.ssh\n```\n\nAfter completing these steps, you should be able to SSH into the server as `ansible_user` using your SSH key, and execute sudo commands without a password prompt.\n\n### 2: Ansible installed on local development machine\n\nTo install Ansible on your local development machine, follow these steps:\n\n1. Create a virtual environment:\n\n```bash\npython3 -m venv ansible-venv\n```\n\n2. Activate the virtual environment:\n\n```bash\nsource ansible-venv/bin/activate\n```\n\n3. Install Ansible within the virtual environment:\n\n```bash\npip install ansible\n```\n\n4. Verify the installation:\n\n```bash\nansible --version\n```\n\nThis approach isolates Ansible and its dependencies in a dedicated environment, preventing conflicts with other Python packages on your system.\n\n## Create Ansible inventory and test connectivity\n\nCreate the following files and directory structure\n\n``` bash\n$ tree physical-server-ansible-playbook\n\u251c\u2500\u2500 ansible.cfg\n\u251c\u2500\u2500 inventory\n\u2502   \u2514\u2500\u2500 hosts\n\u251c\u2500\u2500 playbook.yml\n\u2514\u2500\u2500 roles\n    \u2514\u2500\u2500 hello\n        \u2514\u2500\u2500 tasks\n            \u2514\u2500\u2500 main.yml\n```\n\n```bash\ncat ansible.cfg\n[defaults]\ninventory = inventory/hosts\nremote_user = ansible_user\nprivate_key_file = ~/.ssh/id_ed25519_aslan_ansible\nhost_key_checking = False\ninterpreter_python = auto_silent\n```\n\n```bash\n[lz]\ncat inventory/hosts\naslan ansible_host=aslan\ngreen-lion ansible_host=green-lion\n\n[all:vars]\nansible_user=ansible_user\nansible_ssh_private_key_file=~/.ssh/id_ed25519_aslan_ansible\n```\n\n\nOf note, make sure the ssh key is correct in `ansible.cfg`. Also note that `host_key_checking = False` is a potential security risk. I'm running this on my home LAN so I think I'm good, but just be aware.\n\nI've called this group of servers the \"lz\" for landing zone. I'll continue the metaphor, as I \"land\" on a distant planet and begin \"terraforming.\"\n\n\nTo verify this is working you can run the ansible ping command\n\n```bash\nansible all -m ping                                                                                                                                                                                                                 \u2718 1 master \u2b06 \u2731 \u25fc\ngreen-lion | UNREACHABLE! => {\n    \"changed\": false,\n    \"msg\": \"Failed to connect to the host via ssh: ansible_user@10.0.0.22: Permission denied (publickey).\",\n    \"unreachable\": true\n}\naslan | SUCCESS => {\n    \"ansible_facts\": {\n        \"discovered_interpreter_python\": \"/usr/bin/python3.11\"\n    },\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\n```\n\nAs you can see, my connectivity to `green-lion` is not correct. I'll go ahead and make the ansible user on `green-lion` and try again.\n\n\n```bash\nansible all -m ping                                                                                                                                                                                                                     master \u2b06 \u2731 \u25fc\naslan | SUCCESS => {\n    \"ansible_facts\": {\n        \"discovered_interpreter_python\": \"/usr/bin/python3.11\"\n    },\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\ngreen-lion | SUCCESS => {\n    \"ansible_facts\": {\n        \"discovered_interpreter_python\": \"/usr/bin/python3.10\"\n    },\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\n```\n\n\n## Next Steps\n\nAs you may be able to guess simply from the direction of this blog, I like automating things. In my next post I'll describe how a securely store secrets for usage within Ansible playbooks, and how I create initial random secrets usable for passwords and keys for deployed software.\n\nRead more in [[blog/02-ansible-secrets]].\n\n\n## Untested Sketchy Scripts\n\nI made a script for setting up the Ansible user on a remote machine. This assumes that you have SSH access to an account with sudo permissions on the remote server.\n\nThis may break for various reasons, but it has worked for me.\n\n```Python\n#!/usr/bin/env python3\nimport os\nimport subprocess\nimport sys\nimport getpass\n\ndef run_ssh_command(hostname, command, control_path=None, use_sudo=False):\n    ssh_command = ['ssh']\n    if control_path:\n        ssh_command.extend(['-S', control_path])\n    \n    if use_sudo:\n        full_command = f\"sudo -S bash -c '{command}'\"\n    else:\n        full_command = command\n\n    ssh_command.extend([hostname, full_command])\n    \n    if use_sudo:\n        sudo_password = getpass.getpass(f\"Enter sudo password for {hostname}: \")\n        process = subprocess.Popen(ssh_command, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n        stdout, stderr = process.communicate(input=sudo_password + '\\n')\n    else:\n        result = subprocess.run(ssh_command, capture_output=True, text=True)\n        stdout, stderr = result.stdout, result.stderr\n\n        if result.returncode != 0:\n            print(f\"Error running command: {command}\")\n            print(\"Remote host stderr output:\")\n            print(stderr.strip())\n            raise subprocess.CalledProcessError(result.returncode, ssh_command)\n    return stdout.strip()\n\ndef get_ssh_keys():\n    ssh_dir = os.path.expanduser(\"~/.ssh\")\n    return [f for f in os.listdir(ssh_dir) if f.endswith(\".pub\")]\n\ndef prompt_for_ssh_key(keys):\n    print(\"Available SSH public keys:\")\n    for i, key in enumerate(keys):\n        print(f\"{i + 1}: {key}\")\n    choice = int(input(\"Select the number of the SSH key to use: \")) - 1\n    return keys[choice]\n\ndef check_user_exists(hostname, control_path):\n    return run_ssh_command(hostname, 'id -u ansible_user', control_path).isdigit()\n\ndef check_sudo_permissions(hostname, control_path):\n    return \"NOPASSWD: ALL\" in run_ssh_command(hostname, 'sudo -l -U ansible_user', control_path, use_sudo=True)\n\ndef enable_ansible_user(hostname, ssh_key, control_path):\n    if check_user_exists(hostname, control_path):\n        if check_sudo_permissions(hostname, control_path):\n            print(f\"User ansible_user already exists with appropriate sudo permissions.\")\n            return\n        else:\n            print(f\"User ansible_user exists but does not have appropriate sudo permissions.\")\n            return\n\n    commands = [\n        'adduser --disabled-password --gecos \"\" ansible_user',\n        'echo \"ansible_user ALL=(ALL) NOPASSWD:ALL\" | tee /etc/sudoers.d/ansible_user',\n        'mkdir -p /home/ansible_user/.ssh',\n        'chmod 700 /home/ansible_user/.ssh',\n        'touch /home/ansible_user/.ssh/authorized_keys',\n        'chmod 600 /home/ansible_user/.ssh/authorized_keys',\n        f'echo \"{ssh_key}\" | tee -a /home/ansible_user/.ssh/authorized_keys',\n        'chown -R ansible_user:ansible_user /home/ansible_user/.ssh'\n    ]\n    for command in commands:\n        run_ssh_command(hostname, command, control_path, use_sudo=True)\n\nif __name__ == \"__main__\":\n    if len(sys.argv) != 2:\n        print(\"Usage: enable_ansible <hostname>\")\n        sys.exit(1)\n\n    hostname = sys.argv[1]\n    keys = get_ssh_keys()\n    if not keys:\n        print(\"No SSH public keys found in ~/.ssh\")\n        sys.exit(1)\n\n    selected_key = prompt_for_ssh_key(keys)\n    ssh_key_path = os.path.expanduser(f\"~/.ssh/{selected_key}\")\n    with open(ssh_key_path, 'r') as key_file:\n        ssh_key = key_file.read().strip()\n\n    control_path = f\"/tmp/ansible-ssh-{hostname}-22-control\"\n    \n    # Set up the control master connection\n    subprocess.run(['ssh', '-M', '-S', control_path, '-fNT', hostname], check=True)\n    \n    try:\n        enable_ansible_user(hostname, ssh_key, control_path)\n        print(f\"Ansible user enabled on {hostname} with SSH key {selected_key}\")\n    finally:\n        # Close the control master connection\n        subprocess.run(['ssh', '-S', control_path, '-O', 'exit', hostname], check=True)\n```"
    },
    "/blog": {
        "title": "index",
        "date": "2024-09-20",
        "tags": [],
        "publish": true,
        "url": "/blog",
        "path": "blog/index.md",
        "content": "# Blog\n\n## Subtitle\n\n### Foo\n\n\n```Python\n\ndef foo():\n    print(\"Hello, world!\")\n```"
    },
    "/blog/2024-09-16-deploying-a-basic-django-site-using-ansible": {
        "title": "Deploying A Basic Django Site using Ansible",
        "date": "2024-09-16",
        "tags": [
            "blog",
            "development",
            "infrastructure",
            "ansible"
        ],
        "publish": true,
        "url": "/blog/2024-09-16-deploying-a-basic-django-site-using-ansible",
        "path": "blog/05-simple-ansible-deploy.md",
        "content": "![Silhouette of man magically controlling computer windows](images/django-site-1.webp)\n\nIn this post I describe deploying a simple [Django](https://www.djangoproject.com) based web application into a production environment for use as an online storefront for a small business.\n\n## Background\n\nWhen I'm not [[blog/01-personal-infrastructure|building the new internet]], I am also working on a number of other projects and businesses to support myself. One of these is a small online store selling spiritual themed clothing. For this store, I need a basic website that allows people to buy clothing. I could use a one of many online e-commerce platforms, but I want complete control over the site and features, and am choosing to build it from scratch.\n\nThe store will be built using [Django](https://www.djangoproject.com), and accept payments through [Stripe](https://stripe.com/).\n\nThis project will be a good learning experience, and also demonstrate how the \"legacy\" method of deploying applications can be improved.\n\n## The Plan\n\n### \"Human\" Objectives\n\nI like starting all projects with some idea of what \"success\" looks like. For this project, success means:\n\n1. The online store for this business is up and running well\n2. I have a well-written document describing the process\n3. I know what went well, and what can be improved\n\nSome of these words are pulling a lot of weight. Notably, the phrase \"running well\" can mean many things.\n\nI define it as:\n\n1. People can buy shirts successfully\n2. The website does not go down\n3. If it does go down, I will know about it, and can fix it easily\n4. I can make changes easily without worrying about breaking anything\n\n### Technical Objectives\n\nAt the end of this endeavor, I'll have created a fully functioning system that is easy to change and maintain. It will be self-contained, and not require me to remain engaged with expert-level focus. \n\nI can distill much of what I've learned through the years to: automate everything. By automating everything, I have to explicitly document every step in the form of the \"code\" that does the automation. This forces me to not let anything slip through the cracks, like a one-off hack I find on stack overflow, or that password I set manually, or file permission I needed to change in a panic.\n\nAt the end of this I project I will have:\n\n1. A folder full of python code that is the core Django web application\n2. A Docker container for the application containing a working Django application\n3. A docker container for an `nginx` proxy for serving static files\n4. Secure management of secrets such as API keys and database passwords\n5. A deployment of these containers to real servers, in \"the cloud\" and physically at home\n5. Tools to monitor the running website\n6. Tools to automate all aspects of the deployment\n\n### Assumptions\n\nI will assume that I (or you the reader) has a fully functioning Django application that follows best practices. \n\nSpecifically, I will assume:\n\n1. The Django application does not have any esoteric requirements, and can easily be run locally with `manage.py runserver`\n2. The application does not have and hard-coded keys, credentials, etc, and instead loads these from the operating systems environment\n3. The application does not serve static files directly. These will be served either by a simple web server\n4. The application has logging and error handling setup. Notably, I like using [Sentry](sentry.io) to monitor and keep tabs on my application\n\n\nIn addition, I will assume:\n\n1. This site will be deployed to \"The Cloud\". I will be deploying to Digital Ocean\n2. The site will use an external load balancer that will handle SSL termination. I will again use one from Digital Ocean\n3. The site will use a managed database hosted by the cloud provider\n\nWith these assumptions out of the way, let's get started!\n\nIf you'd like to learn more about Deploying Django, check out the resources:\n\n- https://docs.djangoproject.com/en/5.1/howto/deployment/\n- https://developer.mozilla.org/en-US/docs/Learn/Server-side/Django/Deployment\n\n# Architectural Overview\n\nHere is my go-to architecture for small, low-traffic sites:\n\n1. Frontend load balancer provided by a cloud hosting provider\n2. One or more containerized applications, running Django, with an `nginx` reverse proxy in front of them handling static files. These are orchestrated using `docker compose`.\n3. Managed database provided by cloud hosting provider\n4. Ansible playbook to set everything up\n\nI'll describe the steps I take below in greater detail. This isn't intended to be a tutorial, just a rough overview. \n\n# Steps\n\n### 1. Do the Annoying Manual Steps First\n\nI like getting something working quickly to get momentum going. For this step, I'll go ahead and do the manual steps needed to deploy a site.\n\n1. Ensure you have a domain name. I like getting mine from [Gandi](http://gandi.net)\n2. Provision a \"Droplet\" in Digital Ocean. Configure it to use an SSK key. See [How to Create a Droplet](https://docs.digitalocean.com/products/droplets/how-to/create/) for more information\n3. Provision a  [Load Balancer](https://docs.digitalocean.com/products/networking/load-balancers/) in Digital Ocean. Specifically, I configure it to [terminate SSL](https://docs.digitalocean.com/products/networking/load-balancers/how-to/ssl-termination/), and automatically get new certificates from [Let's Encrypt](https://letsencrypt.org) automatically. This is easy if you [manage your domain](https://docs.digitalocean.com/products/networking/dns/) in Digital Ocean\n4. Connect to your Droplet and do the bare minimum to host something. I like just using `python3 http.server 80` in a folder with a simple `index.html`. You may also want to make a `health` directory with another `index.html` in it so that Digital Ocean will consider this server \"healthy.\"\n5. Configure the Digital Ocean firewall so the web server box is only accessible on port 80 via the load balancer, and your home IP address on port 22 for ssh. Verify you can still SSH into the box, and cannot access it directly via HTTP, while it is still accessible through the load balancer.\n\nHaving done this, I can verify the load balancer and SSL termination are working, and can move on to deploying a working site.\n\n### 2. Setup a Staging Environment\n\nIf you want to go ahead and deploy straight to \"production\", skip this step. I find it very useful to have an environment that is very similar to how I will run the app in production, and use this to test.\n\nIn my case, I have my own custom nginx load balancer, and I setup to SSL terminate and reverse proxy `staging.example.com` \n\nAlternatively, you can spin up another Digital Ocean droplet, and repeat the manual steps with the staging environment.\n\n### 3. Consider Secret Management\n\nFor this site, there are a handful of secrets and configuration values. I keep the non-sensitive ones stored directly in version control, and the sensitive ones encrypted and stored in an [Ansible Vault](https://docs.ansible.com/ansible/latest/vault_guide/index.html) file.\n\nMy Django application itself does not store any secrets, but loads them from [Environment Variables](https://en.wikipedia.org/wiki/Environment_variable)\n\nA Django project loads its runtime settings by importing a [Django settings](https://docs.djangoproject.com/en/5.1/topics/settings/) module. This is a python file with various global variables that are used by other parts of the application.\n\nI have actually created three separate settings modules, a`base` a `local` and a `production`. The `local` and `production` modules import the `base` module, which contains settings that do not change between environments.\n\nThe `local` and `production` modules both use the [python-dotenv](https://pypi.org/project/python-dotenv/) package to load settings from environment variables.\n\nFor example, my `local.py` settings module begins like this:\n\n```Python\nfrom dotenv import load_dotenv\nload_dotenv()\n\nfrom .base import *\n\nDEBUG = True\n\nALLOWED_HOSTS = ['*']\n\n\nSTATIC_URL = '/static/'\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': f'{SITE_NAME}.sqlite',\n    }\n}\n\n```\n\n \n#### Setting Environment Variables at Runtime\n\nTo set environment variables at runtime, the [python-dotenv](https://pypi.org/project/python-dotenv/) looks for a `.env` file in the current working directory, and if it finds one, sets all the described environment variables. For development, I (semi) manually maintain a `.env` file.\n\nFor production, I create a `.env` file during the build process from a template file, and copy it to the production server during the deployment phase. Prior to creating the container, the container runtime (in this case Docker with Docker Compose) loads the the `.env` file and sets the environment variables directly. This avoids having to mount the `.env` file in the Docker container, and removes one Python dependency. \n\nI am currently debating whether I want to forego the [python-dotenv](https://pypi.org/project/python-dotenv/)  library altogether, and use a different tool to load the `.env` on my development system. \n\n\n\n```yaml\nSECRET_KEY={{django_secret_key}}\nDEBUG=False\nPOSTGRES_PORT=5432\nPOSTGRES_HOST=db\nPOSTGRES_USER={{postgres_user}}\nPOSTGRES_PASSWORD={{postgres_password}}\nPOSTGRES_DB={{postgres_db}}\n\nDJANGO_SETTINGS_MODULE={{site_prefix}}.settings.production\nALLOWED_HOSTS={{allowed_hosts}}\n\nADMIN_USERNAME=admin\nADMIN_PASSWORD={{django_admin_password}}\nADMIN_EMAIL={{admin_email}}\n\nEMAIL_HOST={{email_host}}\nEMAIL_PORT={{email_port}}\nEMAIL_USE_TLS={{email_use_tls}}\nEMAIL_HOST_USER={{email_host_user}}\nEMAIL_PASSWORD={{email_password}}\n\nSTRIPE_API_KEY={{stripe_api_key}}\nSTRIPE_PUB_KEY={{stripe_pub_key}}\n```\n\n### 4. Containerize Application including Static File Hosting\n\n I will deploy this application as a [Docker Container](https://docs.docker.com/get-started/docker-concepts/the-basics/what-is-a-container/). I am choosing deploy this as a \"containerized workload\" because I value the determinism and isolation it gives me. Once I create a Docker image, it will run just about the same anywhere, and won't have (much) unintended behavior due to how other services on the server are configured.\n\nAlso, if I'd like to scale up the application, I can simply deploy more containers to more VMs and not have to worry about manual configuration.\n\nI won't write more about the values of containers, but suffice it to say, after many years of resisting, and crying out \"Have operating systems really failed at their key goal of being a platform to run multiple software applications?\", I have decided to embrace the the value they provide, and save my criticism of operating systems for a future discussion.\n\nTo \"containerize\" an application, I consider a few things:\n\n1. What operating system is required to run the software?\n2. What dependencies does it need? \n3. Does it need other services running? Can and should these be containerized as well?\n4. What data from the \"outside\" will the application need? These are settings that will likely live in a `.env` file.\n\nOnce I answer these questions, I whip up a `Dockerfile` that pulls in a good base operating system, and populate the file with all the steps necessary to install the dependencies that are needed, excluding any services that I will run in different containers.\n\nA `Dockerfile` is kind of like a basic shell script that runs a bunch of commands on container, and allows copying between containers. It differs from a traditional shell script in many ways, a notable one being that each step is potentially cached. Keeping that in mind, I typically arrange my `Dockerfile` so most expensive things happen first on their own lines, and fast things that change often are later on in the file, so as I rebuild the image, most of the early steps can be skipped and retrieved from cache. \n\nHere is my `Dockerfile` for this application:\n\n```Dockerfile\n###########\n# Builder #\n###########\n\n# pull official base image\nFROM python:3.11.4-slim-buster as builder\n\n# set work directory\nWORKDIR /usr/src/app\n\n# set environment variables\nENV PYTHONDONTWRITEBYTECODE=1\nENV PYTHONUNBUFFERED=1\n\n# install system dependencies\nRUN apt-get update && \\\n    apt-get install -y --no-install-recommends gcc \n\n# install python dependencies\nCOPY ./frozen-reqs.txt .\nRUN pip wheel --no-cache-dir --no-deps --wheel-dir /usr/src/app/wheels -r frozen-reqs.txt\n\n##############\n# Main Image #\n##############\n\n# pull official base image\nFROM python:3.11.4-slim-buster\n\n# create directory for the app user\nRUN mkdir -p /home/app\n\n# create the app user\nRUN addgroup --system app && adduser --system --group app\n\n# create the appropriate directories\nENV HOME=/home/app\nENV APP_HOME=/home/app/rev\nRUN mkdir -p $APP_HOME/staticfiles $APP_HOME/mediafiles $APP_HOME/logs\nWORKDIR $APP_HOME\n\n# install dependencies\nRUN apt-get update && apt-get install -y --no-install-recommends netcat ffmpeg\n\nCOPY --from=builder /usr/src/app/wheels /wheels\nCOPY --from=builder /usr/src/app/frozen-reqs.txt .\nRUN pip install --upgrade pip\nRUN pip install --no-cache /wheels/*\n\n# install nodejs\nENV NODE_VERSION=20.11.0\nENV NVM_DIR=$HOME/.nvm\n\n# Install curl and nvm\nRUN apt-get update && apt-get install -y curl && \\\n    curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.1/install.sh | bash\n\n# Install Node.js using nvm\nRUN . \"$NVM_DIR/nvm.sh\" && nvm install ${NODE_VERSION} && \\\n    . \"$NVM_DIR/nvm.sh\" && nvm use ${NODE_VERSION} && \\\n    . \"$NVM_DIR/nvm.sh\" && nvm alias default ${NODE_VERSION}\n\n# Update PATH\nENV PATH=\"$NVM_DIR/versions/node/v${NODE_VERSION}/bin:$PATH\"\n\n# copy project\nCOPY . $APP_HOME\n\n# Create log directory and set permissions\nRUN mkdir -p /home/app/logs && chmod -R 755 /home/app/logs\nRUN chown -R app:app /home/app/logs\n\n\nWORKDIR $APP_HOME/store/frontend\nRUN npm install\nWORKDIR $APP_HOME\n\n\n# Ensure log directory is writable\nRUN chmod -R 755 $APP_HOME/logs\n\nRUN sed -i 's/\\r$//g'  $APP_HOME/entrypoint.prod.sh\nRUN chmod +x  $APP_HOME/entrypoint.prod.sh\nRUN chmod +x  $APP_HOME/migrate.sh\n\n# chown all the files to the app user\nRUN chown -R app:app $APP_HOME\n\n# change to the app user\nUSER app\n\n# run entrypoint.prod.sh\nENTRYPOINT [\"/home/app/rev/entrypoint.prod.sh\"]\n```\n\nThere is much to learn about Dockerfiles, including techniques to optimize both the build time, and the final image size. I may take some time to optimize image size later, which is typically done by purging a bunch of files after the work is done, or using multiple images, one to build the application, and another to be the runtime host of it, that doesn't have all the build dependencies. \n\nIn the above file, one optimization I'd like to make is to not include node in the final image. However, I need it since building the frontend bundle happens during the [migration](https://docs.djangoproject.com/en/5.1/topics/migrations/). I will improve this in the future, and copy the optimized bundle into the image, and not bundle node with it.\n\n#### Static File Hosting\n\nDjango sites in production typically do not serve \"static\" files whose content does not change during runtime. Static files are primarily images, css, Javascript and and any other static content. In a Django site, these are added manually to the project, or are the result of compiling a frontend bundle of Javascript, css and more into production bundles.\n\nI typically serve static files from an [nginx](https://nginx.org/en/) server deployed as another container in front of Django. For a high traffic site, it may make sense to copy static files to a CDN or object store like S3. \n\nTo set this up, I create an extremely simple `Dockerfile`\n\n```Dockerfile\nFROM nginx:1.25\n\nRUN rm /etc/nginx/conf.d/default.conf\nCOPY nginx.conf /etc/nginx\n```\n\nThe `nginx.conf` for this container is as follows:\n\n```\nworker_processes 1;\n\nevents {\n    worker_connections 1024;\n}\n\nhttp {\n    include /etc/nginx/mime.types;\n    default_type application/octet-stream;\n\n    gzip on;\n    gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript;\n    gzip_proxied any;\n    gzip_vary on;\n\n    upstream django {\n        server web:8000;\n    }\n\n    server {\n        listen 80;\n\n        location / {\n            proxy_pass http://django;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header Host $host;\n            proxy_redirect off;\n        }\n\n        location /static/ {\n            alias /home/app/{{ app_prefix }}/staticfiles/;\n        }\n\n        location /media/ {\n            alias /home/app/{{ app_prefix }}/mediafiles/;\n        }\n\n        # Add health check endpoint\n        location /health/ {\n            access_log off;\n            return 200 'healthy';\n            add_header Content-Type text/plain;\n        }\n    }\n}\n```\n\nThe main things of not here are \n\n1. Proxy configuration. Location `/` passes all traffic to Django, and sets some headers. \n2. Static configuration. `/static/` serves files directly from `/home/app/{{ app_prefix }}`. The important part here is to copy the static files to the right location prior to running the app\n3. A health check\n\nI'll write about copying files to the right location in later steps.\n\n[Django static files in Production](https://docs.djangoproject.com/en/5.1/howto/static-files/deployment/)\n\n## Surprise Subsection: Using Docker Compose\n\nIn order to manage the Django application and nginx reverse proxy as a single unit, I use [Docker Compose](https://docs.docker.com/compose/) Docker Compose allows me to bring up a few containers together, configure how network traffic passes between them, and [mount](https://docs.docker.com/engine/storage/volumes/) shared directories between containers.\n\nFor static files hosting, the important part is to copy all the static files to a folder (volume in Docker lingo) that is accessible to the nginx container.\n\nRather than explain all the details, here is my `docker-compose.yaml` template, edited slightly for brevity.\n\n```yaml\nversion: '3.8'\n\nservices:\n  web:\n    image: {{image_name}}\n    command: gunicorn {{site_prefix}}.wsgi:application --bind 0.0.0.0:8000 --workers 5 --threads 2\n    volumes:\n      - static_volume:/home/app/{{site_prefix}}/staticfiles\n      - media_volume:/home/app/{{site_prefix}}/mediafiles\n      - logs_volume:/home/app/{{site_prefix}}/logs\n    env_file:\n      - .env\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health/\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n    expose:\n      - 8000\n    depends_on:\n      - db\n      - migration\n\n  db:\n    image: postgres:15\n    volumes:\n      - postgres_data:/var/lib/postgresql/data/\n    env_file:\n      - .env\n\n  nginx:\n    image: {{nginx_image}}\n    container_name: {{site_prefix}}-nginx\n    volumes:\n      - static_volume:/home/app/{{site_prefix}}/staticfiles\n      - media_volume:/home/app/{{site_prefix}}/mediafiles\n    ports:\n      - {{nginx_port}}:80\n    restart: unless-stopped\n    depends_on:\n      - web\n\n  migration:\n    image: {{image_name}}\n    volumes:\n      - static_volume:/home/app/{{site_prefix}}/staticfiles\n      - media_volume:/home/app/{{site_prefix}}/mediafiles\n    command: ./migrate.sh\n    env_file:\n      - .env\n    depends_on:\n      - db\n\nvolumes:\n  postgres_data:\n  static_volume:\n  media_volume:\n  logs_volume:\n```\n\nSome important things to notice:\n\n1. There are volumes defined for postgres, static files, media files, logs. The static files volume is shared by the migration container and the nginx container.\n2. All of them use the same `.env` file.\n\nI will discuss migrations next\n### 5. Setup Migrations\n\nDjango has built in support for database [migrations](https://docs.djangoproject.com/en/5.1/topics/migrations/). In essence, when you change the database layout in code, it is necessary to update the actual database and potentially migrate any data from the old layout to the new one.\n\nAs migrations need to happen before the new application is deployed, I run an ephemeral container every time I push an upgrade. It will connect to the database, and run migrations, and also copy static files to the appropriate place for `nginx` to host them.  \n\n### 6. Create Ansible Playbook to Automate Deployment\n\nOooh fun\n\n### 7. Verify Ansible Deployment on Staging Server\n\nOh yeah\n\n### 8. Configure logging and Error Reporting\nDeepa Foo\n\n### 9. Do this all from CI\n\n\n\n\n# Django deploy\n\n- [x] Ansible vault trickiness. Not sure how env vars are used for getting password. The password storage and multiple envs is messy.\n- [ ] Building docker images in Drone is not ideal. Need to enable caching\n- [ ] Env inflation in staging/prod needs to be figured out. Not working\n- [ ] Certbot thing is a disaster still. Maybe a custom python script or ansible extension is in order...\n\n\n\n\n\n\n\n\n1. Conditional Image Building:\n   You can use Drone's conditional step execution to only rebuild images when certain files have changed. Here's an example:\n\n```yaml\nsteps:\n  - name: build-image\n    image: plugins/docker\n    settings:\n      repo: your-repo/your-image\n      tags: latest\n    when:\n      changeset:\n        - Dockerfile\n        - src/**/*\n```\n\nThis step will only run when the Dockerfile or any file in the src directory has changed.\n\n2. Caching:\n   Drone supports caching to speed up your builds. You can cache Docker layers, dependencies, or any other files. Here's an example:\n\n```yaml\nsteps:\n  - name: restore-cache\n    image: drillster/drone-volume-cache\n    volumes:\n      - name: cache\n        path: /cache\n    settings:\n      restore: true\n      mount:\n        - ./node_modules\n        - ./.npm\n\n  - name: build\n    image: node\n    commands:\n      - npm install\n      - npm run build\n\n  - name: rebuild-cache\n    image: drillster/drone-volume-cache\n    volumes:\n      - name: cache\n        path: /cache\n    settings:\n      rebuild: true\n      mount:\n        - ./node_modules\n        - ./.npm\n\nvolumes:\n  - name: cache\n    host:\n      path: /tmp/cache\n```\n\nThis example caches node_modules and the npm cache.\n\n3. Docker Layer Caching:\n   For Docker builds specifically, you can use Docker's layer caching:\n\n```yaml\nsteps:\n  - name: build-image\n    image: plugins/docker\n    settings:\n      repo: your-repo/your-image\n      tags: latest\n      cache_from: your-repo/your-image:latest\n```\n\nThis tells Docker to use the latest image as a cache source when building.\n\n4. Combine Conditional Building and Caching:\n   You can combine these approaches for optimal efficiency:\n\n```yaml\nsteps:\n  - name: build-image\n    image: plugins/docker\n    settings:\n      repo: your-repo/your-image\n      tags: latest\n      cache_from: your-repo/your-image:latest\n    when:\n      changeset:\n        - Dockerfile\n        - src/**/*\n```\n\nThis will only rebuild the image when relevant files have changed, and it will use layer caching when it does rebuild.\n\nRemember to adjust these examples to fit your specific project structure and needs. Also, ensure that your Drone runner is configured to support the caching mechanism you choose to use.\n\n```\nupstream {{site_name}}_{{env}}_web_servers {\n    least_conn;\n    {% for server in groups['web'] %}\n    server {{ hostvars[server]['ansible_default_ipv4']['address'] }}:{{nginx_port}} max_fails=3 fail_timeout=5s;\n    {% endfor %}\n}\n\nserver {\n    server_name {{domain}};\n\n    location /.well-known/acme-challenge/ {\n        root /var/www/certbot;\n    }\n\n    location / {\n        proxy_pass http://{{site_name}}_{{env}}_web_servers;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n\n    {% if ssl_enabled %}\n    listen 443 ssl; # managed by Certbot\n    ssl_certificate /etc/letsencrypt/live/{{ domain }}/fullchain.pem; # managed by Certbot\n    ssl_certificate_key /etc/letsencrypt/live/{{ domain }}/privkey.pem; # managed by Certbot\n    include /etc/letsencrypt/options-ssl-nginx.conf; # managed by Certbot\n    ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; # managed by Certbot\n    {% else %}\n    listen 80;\n    {% endif %}\n}\n\n{% if ssl_enabled %}\nserver {\n    if ($host = {{ domain }}) {\n        return 301 https://$host$request_uri;\n    } # managed by Certbot\n\n    listen 80;\n    server_name {{ domain }};\n    return 404; # managed by Certbot\n}\n{% endif %}\n```"
    },
    "/blog/2024-09-02-personal-microservices-infrastructure-project": {
        "title": "Personal Microservices Infrastructure Project",
        "date": "2024-09-02",
        "tags": [
            "devops",
            "programming",
            "kubernetes",
            "hashicorp",
            "terraform",
            "homelab"
        ],
        "publish": true,
        "url": "/blog/2024-09-02-personal-microservices-infrastructure-project",
        "path": "blog/01-personal-infrastructure.md",
        "content": "![Image of people building their own digital infrastructure](/images/personal-cloud.png)\n\nJoin me as I build a personal datacenter, a \u201chomelab\u201d suitable for playing with microservices infrastructure. This is the first chapter of a story where people reclaim their power from their digital lords, and grow into equal peers in the digital realm.\n\nI am starting my portion of this journey by arranging computer and software components to build a datacenter to serve as the foundation of a \"Personal Cloud.\" This personal cloud will create my own digital sovereign territory from which I can steward my information and define my interaction with my peers. This personal cloud is intended as a proof of concept of what a better internet could look like.\n\n### Introduction\n\nThere was a time when only a few wealthy people had books. There was a time when only a few people had telephones. There was a time when only a few people had computers. We are now in an era where many people have easy access to these modern information technologies. In this era, there are only a few people who have their own datacenter. Everything is in \u201cthe cloud\u201d, a new form of digital feudalism reigns supreme. Most people life as serfs, graciously accepting the few crumbs of value that trickle down in the form of apps and services while those people controlling them accumulate wealth and power.\n\n## The Nitty Gritty\n\nThis project aims to create a comprehensive, real-world microservices architecture, focusing on automation, scalability, and best practices in modern DevOps.\n\nI'll write developer journal entries as blog posts, as well as organize them into a more structured \"how-to\" documentation suitable for recreating the project or following along. I\u2019ll also write the occasional essay about the meaning and \u2018philosophy\u2019 motivating my actions.\u00a0\n\nAs of now, I've done some preliminary research and have come up with this plan of action. As I go ahead and my hands dirty, I imagine some of this plan will change, either slightly or dramatically as I learn from my mistakes and refine my understanding.\n\n### Tentative Plan of Action\n\n#### 1. Physical Server Setup\n  - [[blog/01-ansible-setup]]\n  - [[blog/02-ansible-secrets]]\n  - [[blog/03-simple-automation]]\n  - [[blog/04-initial-secrets]]\n\n#### 2. CI Runner Setup:\n   - Install and configure a CI tool (likely [Drone CI](https://www.drone.io/))\n   - Migrate legacy services to Ansible, deployed by CI\n   - Automatically run Ansible on infrastructure code changes.\n\n#### 3. Core Services\nIn this phase, I will deploy a small cluster of virtual machines that will host \u201ccore services\u201d that will make deploying production services easier and more repeatable.\n   - Use [Packer](https://www.packer.io/) for creating standardized VM images\n   - Implement [Terraform](https://www.terraform.io/) for infrastructure provisioning\n   - Setup [Headscale](https://www.headscale.net/) for secure networking\n   - Set up [Nomad](https://www.nomadproject.io/) for initial workload orchestration\n   - Deploy [Vault](https://www.vaultproject.io/) for secrets management\n   - Set up monitoring tools (e.g., [Prometheus](https://prometheus.io/), [Grafana](https://grafana.com/))\n   - Implement logging solution (e.g., [ELK Stack](https://www.elastic.co/elastic-stack))\n\n#### 4. Advanced Orchestration:\n   - Create a separate Nomad cluster for application workloads\n   - Potentially set up [K3s](https://k3s.io/) (lightweight Kubernetes) within Nomad\n\n#### 5. Network and Security:\n   - Implement ingress controllers and load balancers\n   - Set up network policies and firewalls\n\n#### 6. Automation and Scalability:\n   - Develop scripts and workflows for rapid cluster creation\n   - Implement auto-scaling and self-healing capabilities\n#### 7. Begin deploying Services\n\nIn this phase, I will begin with the working foundation to deploy modern applications that I\u2019ve build in subsequent steps. From here I will begin to make this cloud more personal, and develop applications and services on top of it that are valuable to me, and demonstrate the value of a personal cloud.\n\n### Learning Objectives\n\nWhy am I doing this? I'd like to learn these tools, and build something useful for myself and others. I have a large project in mind, and I will discuss it after I build the initial proof of concept \"personal cloud\".\n\nPut into dry bullet points, I'd like to:\n\n- Gain hands-on experience with modern DevOps tools and practices\n- Understand whether microservices architecture is a good fit for my project\n- Develop skills in automation, security, and scalability in distributed systems\n- Build something useful for myself and others\n\n### Sharing Knowledge\n\nThroughout this project, I will:\n\n  - Document each step in this developer journal or blog\n  - Create some guides and tutorials\n  - Share challenges faced and solutions implemented. I'd like to specifically share my perspective, from someone new to \n    modern DevOps, though no stranger to software engineering.\n  - Possibly create video content"
    }
}